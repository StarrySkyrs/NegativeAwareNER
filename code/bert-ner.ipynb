{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T14:38:43.352181Z","iopub.execute_input":"2022-05-18T14:38:43.352794Z","iopub.status.idle":"2022-05-18T14:38:43.381386Z","shell.execute_reply.started":"2022-05-18T14:38:43.352705Z","shell.execute_reply":"2022-05-18T14:38:43.380581Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nertag/clean_data.csv\n/kaggle/input/nertag/unnegated_File_Name.csv\n/kaggle/input/nertag/un-negated_clean_data.csv\n/kaggle/input/nertag/File_Name.csv\n/kaggle/input/nertag/reannotated_val.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Changing the transformers and tokenizer versions","metadata":{}},{"cell_type":"code","source":"pip install tokenizers==0.8.0rc4","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:38:52.860566Z","iopub.execute_input":"2022-05-18T14:38:52.860827Z","iopub.status.idle":"2022-05-18T14:39:06.030965Z","shell.execute_reply.started":"2022-05-18T14:38:52.860800Z","shell.execute_reply":"2022-05-18T14:39:06.029200Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tokenizers==0.8.0rc4\n  Downloading tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 4.18.0 requires tokenizers!=0.11.3,<0.13,>=0.11.1, but you have tokenizers 0.8.0rc4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.8.0rc4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers==3.0.1","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:39:12.009392Z","iopub.execute_input":"2022-05-18T14:39:12.009981Z","iopub.status.idle":"2022-05-18T14:39:26.178856Z","shell.execute_reply.started":"2022-05-18T14:39:12.009939Z","shell.execute_reply":"2022-05-18T14:39:26.178035Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting transformers==3.0.1\n  Downloading transformers-3.0.1-py3-none-any.whl (757 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (2.27.1)\nRequirement already satisfied: tokenizers==0.8.0-rc4 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (0.8.0rc4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (4.63.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (2021.11.10)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (0.1.96)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (0.0.53)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (3.6.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.1) (3.0.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (1.26.8)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.1) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.1) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.1) (8.0.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses->transformers==3.0.1) (4.11.3)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==3.0.1) (4.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==3.0.1) (3.7.0)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.18.0\n    Uninstalling transformers-4.18.0:\n      Successfully uninstalled transformers-4.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.9.3 requires transformers<4.19,>=4.1, but you have transformers 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed transformers-3.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\ntransformers.__version__","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:39:31.335362Z","iopub.execute_input":"2022-05-18T14:39:31.335830Z","iopub.status.idle":"2022-05-18T14:39:39.714952Z","shell.execute_reply.started":"2022-05-18T14:39:31.335783Z","shell.execute_reply":"2022-05-18T14:39:39.714147Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\nThe current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"The current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'3.0.1'"},"metadata":{}}]},{"cell_type":"code","source":"import tokenizers\ntokenizers.__version__","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:39:43.240933Z","iopub.execute_input":"2022-05-18T14:39:43.241216Z","iopub.status.idle":"2022-05-18T14:39:43.249147Z","shell.execute_reply.started":"2022-05-18T14:39:43.241182Z","shell.execute_reply":"2022-05-18T14:39:43.248384Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'0.8.0.rc4'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,AutoConfig","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:39:47.817880Z","iopub.execute_input":"2022-05-18T14:39:47.818438Z","iopub.status.idle":"2022-05-18T14:39:47.824055Z","shell.execute_reply.started":"2022-05-18T14:39:47.818390Z","shell.execute_reply":"2022-05-18T14:39:47.823210Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Reading data","metadata":{}},{"cell_type":"code","source":"un_neg_data= pd.read_csv(\"../input/nertag/unnegated_File_Name.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:39:54.730374Z","iopub.execute_input":"2022-05-18T14:39:54.731038Z","iopub.status.idle":"2022-05-18T14:39:54.746807Z","shell.execute_reply.started":"2022-05-18T14:39:54.731003Z","shell.execute_reply":"2022-05-18T14:39:54.746141Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ndata = pd.read_csv(\"../input/nertag/File_Name.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:39:58.005716Z","iopub.execute_input":"2022-05-18T14:39:58.005984Z","iopub.status.idle":"2022-05-18T14:39:58.015319Z","shell.execute_reply.started":"2022-05-18T14:39:58.005956Z","shell.execute_reply":"2022-05-18T14:39:58.014477Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:04.339558Z","iopub.execute_input":"2022-05-18T14:40:04.339877Z","iopub.status.idle":"2022-05-18T14:40:04.360078Z","shell.execute_reply.started":"2022-05-18T14:40:04.339843Z","shell.execute_reply":"2022-05-18T14:40:04.359398Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                              Sentence  \\\n0    I am looking for a black gloss 33 inch firecla...   \n1    Looking for pre workout Pump addict instead of...   \n2    i need a 48 inch glass sliding goof and a show...   \n3    Hello, do any of your free standing tubs have ...   \n4    I'm looking for a 24 inch white mirror that is...   \n..                                                 ...   \n162       What rectangular shower units are available?   \n163  I am looking for a 35 inch bathroom sink count...   \n164  I'm looking f or a Black Matte bathtub double ...   \n165               Need a 27 inch frameless shower door   \n166                 Looking for 30 inch white desk set   \n\n                                                  Tags  \n0    O,O,O,O,O,B-COLOUR,B-TEXTURE,B-SIZE,I-SIZE,B-P...  \n1    O,O,B-PRODUCT,I-PRODUCT,I-PRODUCT,I-PRODUCT,O,...  \n2    O,O,O,B-SIZE,I-SIZE,B-MATERIAL,B-PRODUCT,I-PRO...  \n3    O,O,O,O,O,B-ATTRIBUTE,I-ATTRIBUTE,B-PRODUCT,O,...  \n4    O,O,O,O,B-SIZE,I-SIZE,B-COLOUR,B-PRODUCT,O,O,B...  \n..                                                 ...  \n162                  O,B-SHAPE,B-PRODUCT,I-PRODUCT,O,O  \n163  O,O,O,O,O,B-SIZE,I-SIZE,B-PRODUCT,I-PRODUCT,I-...  \n164  O,O,O,O,O,B-COLOUR,B-TEXTURE,B-PRODUCT,I-PRODU...  \n165  O,O,B-SIZE,I-SIZE,B-N-ATTRIBUTE,B-PRODUCT,I-PR...  \n166     O,O,B-SIZE,I-SIZE,B-COLOUR,B-PRODUCT,I-PRODUCT  \n\n[167 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I am looking for a black gloss 33 inch firecla...</td>\n      <td>O,O,O,O,O,B-COLOUR,B-TEXTURE,B-SIZE,I-SIZE,B-P...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Looking for pre workout Pump addict instead of...</td>\n      <td>O,O,B-PRODUCT,I-PRODUCT,I-PRODUCT,I-PRODUCT,O,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i need a 48 inch glass sliding goof and a show...</td>\n      <td>O,O,O,B-SIZE,I-SIZE,B-MATERIAL,B-PRODUCT,I-PRO...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hello, do any of your free standing tubs have ...</td>\n      <td>O,O,O,O,O,B-ATTRIBUTE,I-ATTRIBUTE,B-PRODUCT,O,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I'm looking for a 24 inch white mirror that is...</td>\n      <td>O,O,O,O,B-SIZE,I-SIZE,B-COLOUR,B-PRODUCT,O,O,B...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>What rectangular shower units are available?</td>\n      <td>O,B-SHAPE,B-PRODUCT,I-PRODUCT,O,O</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>I am looking for a 35 inch bathroom sink count...</td>\n      <td>O,O,O,O,O,B-SIZE,I-SIZE,B-PRODUCT,I-PRODUCT,I-...</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>I'm looking f or a Black Matte bathtub double ...</td>\n      <td>O,O,O,O,O,B-COLOUR,B-TEXTURE,B-PRODUCT,I-PRODU...</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>Need a 27 inch frameless shower door</td>\n      <td>O,O,B-SIZE,I-SIZE,B-N-ATTRIBUTE,B-PRODUCT,I-PR...</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>Looking for 30 inch white desk set</td>\n      <td>O,O,B-SIZE,I-SIZE,B-COLOUR,B-PRODUCT,I-PRODUCT</td>\n    </tr>\n  </tbody>\n</table>\n<p>167 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"secondary_data = pd.read_csv(\"../input/nertag/clean_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:09.118286Z","iopub.execute_input":"2022-05-18T14:40:09.118763Z","iopub.status.idle":"2022-05-18T14:40:09.133461Z","shell.execute_reply.started":"2022-05-18T14:40:09.118725Z","shell.execute_reply":"2022-05-18T14:40:09.132602Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"unneg_secondary_data= pd.read_csv(\"../input/nertag/un-negated_clean_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:12.338044Z","iopub.execute_input":"2022-05-18T14:40:12.338331Z","iopub.status.idle":"2022-05-18T14:40:12.351945Z","shell.execute_reply.started":"2022-05-18T14:40:12.338301Z","shell.execute_reply":"2022-05-18T14:40:12.351292Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"secondary_data","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:17.138287Z","iopub.execute_input":"2022-05-18T14:40:17.139128Z","iopub.status.idle":"2022-05-18T14:40:17.150757Z","shell.execute_reply.started":"2022-05-18T14:40:17.139091Z","shell.execute_reply":"2022-05-18T14:40:17.149730Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"       Tokens       Tags  sentence\n0           I          O         0\n1          am          O         0\n2     looking          O         0\n3         for          O         0\n4           a          O         0\n...       ...        ...       ...\n2345       30     B-SIZE       166\n2346     inch     I-SIZE       166\n2347    white   B-COLOUR       166\n2348     desk  B-PRODUCT       166\n2349      set  I-PRODUCT       166\n\n[2350 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Tags</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>am</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>looking</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>for</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>30</td>\n      <td>B-SIZE</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2346</th>\n      <td>inch</td>\n      <td>I-SIZE</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>white</td>\n      <td>B-COLOUR</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>desk</td>\n      <td>B-PRODUCT</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>set</td>\n      <td>I-PRODUCT</td>\n      <td>166</td>\n    </tr>\n  </tbody>\n</table>\n<p>2350 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"labels_to_ids = {k: v for v, k in enumerate(secondary_data.Tags.unique())}\nids_to_labels = {v: k for v, k in enumerate(secondary_data.Tags.unique())}\nlabels_to_ids","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:30.370109Z","iopub.execute_input":"2022-05-18T14:40:30.370413Z","iopub.status.idle":"2022-05-18T14:40:30.386319Z","shell.execute_reply.started":"2022-05-18T14:40:30.370380Z","shell.execute_reply":"2022-05-18T14:40:30.385614Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'O': 0,\n 'B-COLOUR': 1,\n 'B-TEXTURE': 2,\n 'B-SIZE': 3,\n 'I-SIZE': 4,\n 'B-PRODUCT': 5,\n 'I-PRODUCT': 6,\n 'B-N-PRODUCT': 7,\n 'I-N-PRODUCT': 8,\n 'B-MATERIAL': 9,\n 'B-PRICE': 10,\n 'I-PRICE': 11,\n 'B-ATTRIBUTE': 12,\n 'I-ATTRIBUTE': 13,\n 'B-N-TEXTURE': 14,\n 'I-MATERIAL': 15,\n 'I-COLOUR': 16,\n 'B-N-ATTRIBUTE': 17,\n 'B-SHAPE': 18,\n 'I-N-ATTRIBUTE': 19,\n 'B-N-COLOUR': 20,\n 'I-N-COLOUR': 21,\n 'B-N-SHAPE': 22,\n 'B-N-MATERIAL': 23,\n 'B-N-SIZE': 24,\n 'I-N-SIZE': 25,\n 'B-N-PRICE': 26,\n 'I-N-PRICE': 27}"},"metadata":{}}]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 128\nVALID_BATCH_SIZE = 2\nEPOCHS = 10\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:36.093199Z","iopub.execute_input":"2022-05-18T14:40:36.093805Z","iopub.status.idle":"2022-05-18T14:40:38.745584Z","shell.execute_reply.started":"2022-05-18T14:40:36.093759Z","shell.execute_reply":"2022-05-18T14:40:38.744854Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73eefaf19cf440bb426f75e0932aef4"}},"metadata":{}}]},{"cell_type":"code","source":"class dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n  def __getitem__(self, index):\n        # step 1: get the sentence and word labels \n        sentence = self.data.Sentence[index].strip().split()  \n        word_labels = self.data.Tags[index].split(\",\") \n\n        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n        encoding = self.tokenizer(sentence,\n                             is_pretokenized=True, \n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        # step 3: create token labels only for first word pieces of each tokenized word\n        labels = [labels_to_ids[label] for label in word_labels] \n        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n        # create an empty array of -100 of length max_length\n        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n        \n        # set only labels whose first offset position is 0 and the second is not 0\n        i = 0\n        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n          if mapping[0] == 0 and mapping[1] != 0:\n            # overwrite label\n            encoded_labels[idx] = labels[i]\n            i += 1\n\n        # step 4: turn everything into PyTorch tensors\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        item['labels'] = torch.as_tensor(encoded_labels)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:40:42.762841Z","iopub.execute_input":"2022-05-18T14:40:42.763125Z","iopub.status.idle":"2022-05-18T14:40:42.775740Z","shell.execute_reply.started":"2022-05-18T14:40:42.763095Z","shell.execute_reply":"2022-05-18T14:40:42.774952Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\",use_fast=True)\n# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")","metadata":{"execution":{"iopub.status.busy":"2022-05-16T15:31:26.293785Z","iopub.execute_input":"2022-05-16T15:31:26.294571Z","iopub.status.idle":"2022-05-16T15:31:30.658826Z","shell.execute_reply.started":"2022-05-16T15:31:26.294521Z","shell.execute_reply":"2022-05-16T15:31:30.658113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = 0.9\ntrain_dataset = data.sample(frac=train_size,random_state=200)\ntest_dataset = data.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = dataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = dataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:41:17.911694Z","iopub.execute_input":"2022-05-18T14:41:17.911959Z","iopub.status.idle":"2022-05-18T14:41:17.923820Z","shell.execute_reply.started":"2022-05-18T14:41:17.911931Z","shell.execute_reply":"2022-05-18T14:41:17.922951Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"FULL Dataset: (167, 2)\nTRAIN Dataset: (150, 2)\nTEST Dataset: (17, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"training_set[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:41:23.487269Z","iopub.execute_input":"2022-05-18T14:41:23.487953Z","iopub.status.idle":"2022-05-18T14:41:23.503898Z","shell.execute_reply.started":"2022-05-18T14:41:23.487917Z","shell.execute_reply":"2022-05-18T14:41:23.503213Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101,  1045,  2215,  2074,  2023,  5239,  2302,  1996,  2327,  2004,\n          1045,  2215,  2327,  5614, 22739,  2006,  1037,  3302,  9844,  2026,\n          3829,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'offset_mapping': tensor([[0, 0],\n         [0, 1],\n         [0, 4],\n         [0, 4],\n         [0, 4],\n         [0, 7],\n         [0, 7],\n         [0, 3],\n         [0, 3],\n         [0, 2],\n         [0, 1],\n         [0, 4],\n         [0, 3],\n         [0, 7],\n         [0, 6],\n         [0, 2],\n         [0, 1],\n         [0, 7],\n         [0, 8],\n         [0, 2],\n         [0, 7],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0],\n         [0, 0]]),\n 'labels': tensor([-100,    0,    0,    0,    0,    5,    0,    0,   17,    0,    0,    0,\n            5,    6,    6,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100])}"},"metadata":{}}]},{"cell_type":"code","source":"for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n  print('{0:10}  {1}'.format(token, label))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:41:32.298451Z","iopub.execute_input":"2022-05-18T14:41:32.298798Z","iopub.status.idle":"2022-05-18T14:41:32.323773Z","shell.execute_reply.started":"2022-05-18T14:41:32.298765Z","shell.execute_reply":"2022-05-18T14:41:32.323123Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[CLS]       -100\ni           0\nwant        0\njust        0\nthis        0\ncabinet     5\nwithout     0\nthe         0\ntop         17\nas          0\ni           0\nwant        0\ntop         5\nmounted     6\nbasins      6\non          0\na           0\nsurface     0\nmatching    0\nmy          0\nkitchen     0\n[SEP]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n[PAD]       -100\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:41:38.655561Z","iopub.execute_input":"2022-05-18T14:41:38.656000Z","iopub.status.idle":"2022-05-18T14:41:38.661073Z","shell.execute_reply.started":"2022-05-18T14:41:38.655963Z","shell.execute_reply":"2022-05-18T14:41:38.660161Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:41:41.510592Z","iopub.execute_input":"2022-05-18T14:41:41.510873Z","iopub.status.idle":"2022-05-18T14:41:41.574074Z","shell.execute_reply.started":"2022-05-18T14:41:41.510843Z","shell.execute_reply":"2022-05-18T14:41:41.573374Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model Generation","metadata":{}},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:41:49.130067Z","iopub.execute_input":"2022-05-18T14:41:49.130604Z","iopub.status.idle":"2022-05-18T14:42:37.371822Z","shell.execute_reply.started":"2022-05-18T14:41:49.130561Z","shell.execute_reply":"2022-05-18T14:42:37.371168Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29eca3cf40d14cb69784274a6ce6288a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61da73a085f44b6beab0351b8732239"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=28, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import BertForSequenceClassification, BertConfig\n\n# config = AutoConfig.from_pretrained(\"dslim/bert-base-NER\")\n# config.num_labels = 28\n# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", config =config)\n# model.parameters","metadata":{"execution":{"iopub.status.busy":"2022-05-16T15:43:33.058849Z","iopub.execute_input":"2022-05-16T15:43:33.059178Z","iopub.status.idle":"2022-05-16T15:43:36.625781Z","shell.execute_reply.started":"2022-05-16T15:43:33.059146Z","shell.execute_reply":"2022-05-16T15:43:36.624777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = training_set[2]\ninput_ids = inputs[\"input_ids\"].unsqueeze(0)\nattention_mask = inputs[\"attention_mask\"].unsqueeze(0)\nlabels = inputs[\"labels\"].unsqueeze(0)\n\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\nlabels = labels.to(device)\n\noutputs = model(input_ids, attention_mask=attention_mask, labels=labels)\ninitial_loss = outputs[0]\ninitial_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:42:47.400416Z","iopub.execute_input":"2022-05-18T14:42:47.400894Z","iopub.status.idle":"2022-05-18T14:42:48.281461Z","shell.execute_reply.started":"2022-05-18T14:42:47.400856Z","shell.execute_reply":"2022-05-18T14:42:48.280767Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor(3.2155, device='cuda:0', grad_fn=<NllLossBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"tr_logits = outputs[1]\ntr_logits.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:42:50.430657Z","iopub.execute_input":"2022-05-18T14:42:50.431305Z","iopub.status.idle":"2022-05-18T14:42:50.439422Z","shell.execute_reply.started":"2022-05-18T14:42:50.431269Z","shell.execute_reply":"2022-05-18T14:42:50.438596Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128, 28])"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:42:52.989082Z","iopub.execute_input":"2022-05-18T14:42:52.989372Z","iopub.status.idle":"2022-05-18T14:42:52.995743Z","shell.execute_reply.started":"2022-05-18T14:42:52.989324Z","shell.execute_reply":"2022-05-18T14:42:52.995012Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\ndef train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 100==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 100 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_labels.extend(labels)\n        tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:42:58.282938Z","iopub.execute_input":"2022-05-18T14:42:58.283327Z","iopub.status.idle":"2022-05-18T14:42:58.296895Z","shell.execute_reply.started":"2022-05-18T14:42:58.283290Z","shell.execute_reply":"2022-05-18T14:42:58.295426Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:43:02.161914Z","iopub.execute_input":"2022-05-18T14:43:02.162730Z","iopub.status.idle":"2022-05-18T14:43:19.283215Z","shell.execute_reply.started":"2022-05-18T14:43:02.162689Z","shell.execute_reply":"2022-05-18T14:43:19.282489Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Training epoch: 1\nTraining loss per 100 training steps: 3.186508893966675\nTraining loss epoch: 3.137860417366028\nTraining accuracy epoch: 0.17492283112542176\nTraining epoch: 2\nTraining loss per 100 training steps: 2.98828387260437\nTraining loss epoch: 2.937563419342041\nTraining accuracy epoch: 0.4587003677802354\nTraining epoch: 3\nTraining loss per 100 training steps: 2.7945079803466797\nTraining loss epoch: 2.7456114292144775\nTraining accuracy epoch: 0.5746918265670986\nTraining epoch: 4\nTraining loss per 100 training steps: 2.603875160217285\nTraining loss epoch: 2.534140110015869\nTraining accuracy epoch: 0.6149791408276017\nTraining epoch: 5\nTraining loss per 100 training steps: 2.4134159088134766\nTraining loss epoch: 2.2988189458847046\nTraining accuracy epoch: 0.6362070243688907\nTraining epoch: 6\nTraining loss per 100 training steps: 2.2096762657165527\nTraining loss epoch: 2.138060450553894\nTraining accuracy epoch: 0.613853075476766\nTraining epoch: 7\nTraining loss per 100 training steps: 2.0349888801574707\nTraining loss epoch: 1.9735791683197021\nTraining accuracy epoch: 0.61630345639851\nTraining epoch: 8\nTraining loss per 100 training steps: 1.8642551898956299\nTraining loss epoch: 1.933466911315918\nTraining accuracy epoch: 0.5806534622582658\nTraining epoch: 9\nTraining loss per 100 training steps: 1.790836215019226\nTraining loss epoch: 1.7693960070610046\nTraining accuracy epoch: 0.6121930167415803\nTraining epoch: 10\nTraining loss per 100 training steps: 1.7114940881729126\nTraining loss epoch: 1.6980406641960144\nTraining accuracy epoch: 0.6052381601230218\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['input_ids'].to(device, dtype = torch.long)\n            mask = batch['attention_mask'].to(device, dtype = torch.long)\n            labels = batch['labels'].to(device, dtype = torch.long)\n            \n            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n            \n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += labels.size(0)\n        \n            if idx % 100==0:\n                loss_step = eval_loss/nb_eval_steps\n                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # compute evaluation accuracy\n            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            \n            # only compute accuracy at active labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            \n            eval_labels.extend(labels)\n            eval_preds.extend(predictions)\n            \n            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n\n    labels = [ids_to_labels[id.item()] for id in eval_labels]\n    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:43:26.252176Z","iopub.execute_input":"2022-05-18T14:43:26.252638Z","iopub.status.idle":"2022-05-18T14:43:26.263977Z","shell.execute_reply.started":"2022-05-18T14:43:26.252603Z","shell.execute_reply":"2022-05-18T14:43:26.263312Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"labels, predictions = valid(model, testing_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T14:43:32.774686Z","iopub.execute_input":"2022-05-18T14:43:32.775106Z","iopub.status.idle":"2022-05-18T14:43:32.913221Z","shell.execute_reply.started":"2022-05-18T14:43:32.775067Z","shell.execute_reply":"2022-05-18T14:43:32.912124Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Validation loss per 100 evaluation steps: 1.4235080480575562\nValidation Loss: 1.56599850124783\nValidation Accuracy: 0.6313417913328307\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:43:17.219622Z","iopub.execute_input":"2022-05-16T14:43:17.220429Z","iopub.status.idle":"2022-05-16T14:43:31.664489Z","shell.execute_reply.started":"2022-05-16T14:43:17.220383Z","shell.execute_reply":"2022-05-16T14:43:31.663611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\nprint(classification_report(labels, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T14:43:36.51085Z","iopub.execute_input":"2022-05-16T14:43:36.511215Z","iopub.status.idle":"2022-05-16T14:43:36.565174Z","shell.execute_reply.started":"2022-05-16T14:43:36.511174Z","shell.execute_reply":"2022-05-16T14:43:36.564176Z"},"trusted":true},"execution_count":null,"outputs":[]}]}