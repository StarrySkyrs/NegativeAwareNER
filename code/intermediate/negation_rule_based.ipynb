{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e93d43-0cd8-4678-93f0-6503a9eb5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c20571-9315-4e47-a690-07d67e601713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (60.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\miniconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\andre\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\andre\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\andre\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\andre\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\andre\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\andre\\miniconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5ed181-378b-4533-9e92-8d0c0677abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013552a7-6734-4fc8-8383-191d496b0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/ner_tagged.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad220f8-ebed-4eaf-b434-0d202f4b045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'O':'Tags','-DOCSTART-':'Tokens','-X-':'X'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0e7e76-0723-4466-9bb8-078937693787",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data.reset_index().iterrows():\n",
    "    if type(row.Tokens) == str:\n",
    "        if row.Tokens.lower().startswith(\"un\") and \"-N-\" in row.Tags and row.Tokens != \"undermounted\":\n",
    "            data.at[i,'Tags'] = row.Tags.replace(\"N-\", \"\")\n",
    "    if type(row.Tokens) == str:\n",
    "        if row.Tokens.lower().endswith(\"less\") or row.Tokens.lower() == \"screw\" or row.Tokens.lower().endswith(\"less.\") and \"-N-\" in row.Tags:\n",
    "            data.at[i,'Tags'] = row.Tags.replace(\"N-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2ff45b-b61a-4763-82be-121dded04bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "is_negative = []\n",
    "for i, row in data.reset_index().iterrows():\n",
    "    if type(row.Tokens) == str:\n",
    "        if \"-N-\" in row.Tags:\n",
    "            is_negative.append(True)\n",
    "            data.at[i, \"Tags\"] = row.Tags.replace(\"N-\", \"\")\n",
    "        else:\n",
    "            is_negative.append(False)\n",
    "    else:\n",
    "        is_negative.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694343e3-188b-4813-bcdc-5e93967bba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"is_negative\"] = is_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6caf07ea-2289-4853-9d27-92c83e05a05c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_triples(df):\n",
    "    \n",
    "    current_sent = []\n",
    "    \n",
    "    all_sents = []\n",
    "    \n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        if type(row.Tags) == str:\n",
    "            current_sent.append((row.Tokens, row.Tags, row.is_negative))\n",
    "        else:\n",
    "            all_sents.append(current_sent)\n",
    "            current_sent = []\n",
    "            \n",
    "    return(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae51310-c292-4bcf-973f-901b98d639f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_io(triples):\n",
    "    for sent in triples:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word[1].startswith(\"B\"):\n",
    "                sent[i] = (word[0], \"I\" + word[1][1:], word[2])\n",
    "                \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "169bb733-8606-45e1-8ccd-c2848de4ae6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_sents = change_to_io(get_triples(data))\n",
    "\n",
    "neg_sents = []\n",
    "\n",
    "for sent in all_sents:\n",
    "    for word, iob, neg in sent:\n",
    "        if neg:\n",
    "            neg_sents.append(sent)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "627c8c59-1bbc-45f2-9bcd-70073fdbe7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_sentence(sent, for_evaluation=False):  # If -B- is among the target labels.\n",
    "    \n",
    "#     list_counter = 0\n",
    "\n",
    "#     final_list = []\n",
    "\n",
    "#     current_list = []\n",
    "\n",
    "#     entity_indices = []\n",
    "    \n",
    "#     pos_indices = []\n",
    "    \n",
    "#     neg_indices = []\n",
    "    \n",
    "#     pos_ent_names = []\n",
    "    \n",
    "#     neg_ent_names = []\n",
    "    \n",
    "#     for i in range(1, len(sent)):\n",
    "#         first_word = sent[i-1][0]\n",
    "#         first_iob = sent[i-1][1][0]\n",
    "#         second_word = sent[i][0]\n",
    "#         second_iob = sent[i][1][0]\n",
    "#         if for_evaluation:\n",
    "#             first_neg = sent[i-1][2]\n",
    "#             second_neg = sent[i][2]\n",
    "\n",
    "#         if len(current_list) == 0:\n",
    "#             current_list.append(first_word)\n",
    "\n",
    "#         if (first_iob == second_iob or second_iob == \"I\") and second_iob != \"B\":\n",
    "#             current_list.append(second_word)\n",
    "\n",
    "#         else:\n",
    "#             if first_iob != \"O\":\n",
    "#                 entity_indices.append(list_counter)\n",
    "#                 if for_evaluation:\n",
    "#                     if first_neg:\n",
    "#                         neg_indices.append(list_counter)\n",
    "#                     else:\n",
    "#                         pos_indices.append(list_counter)\n",
    "\n",
    "#             final_list.append(current_list)\n",
    "#             current_list = []\n",
    "#             list_counter += 1\n",
    "\n",
    "#             if i == len(sent)-1:\n",
    "#                 final_list.append([second_word])\n",
    "#                 if second_iob == \"B\":\n",
    "#                     entity_indices.append(list_counter)\n",
    "#                     if for_evaluation:\n",
    "#                         if second_neg:\n",
    "#                             neg_indices.append(list_counter)\n",
    "#                         else:\n",
    "#                             pos_indices.append(list_counter)\n",
    "\n",
    "#     if current_list != [] and second_iob == \"I\":\n",
    "#         final_list.append(current_list)\n",
    "#         entity_indices.append(list_counter)\n",
    "#         if for_evaluation:\n",
    "#             if second_neg == True:\n",
    "#                 neg_indices.append(list_counter)\n",
    "#             else:\n",
    "#                 pos_indices.append(list_counter)\n",
    "        \n",
    "#     if for_evaluation:\n",
    "#         for ent_idx in pos_indices:\n",
    "#             for i, phrase in enumerate(final_list):\n",
    "#                 if i == ent_idx:\n",
    "#                     pos_ent_names.append(\" \".join(word for word in final_list[i]))\n",
    "\n",
    "#         for ent_idx in neg_indices:\n",
    "#             for i, phrase in enumerate(final_list):\n",
    "#                 if i == ent_idx:\n",
    "#                     neg_ent_names.append(\" \".join(word for word in final_list[i]))\n",
    "                    \n",
    "#         return pos_ent_names, neg_ent_names\n",
    "    \n",
    "#     else:\n",
    "#         if current_list != [] and current_list != final_list[-1]:\n",
    "#             final_list.append(current_list)\n",
    "#         return final_list, entity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce1849a-474b-4247-8595-0e4d93b25e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(sent, for_evaluation=False):\n",
    "    \n",
    "    list_counter = 0\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    current_list = []\n",
    "\n",
    "    entity_indices = []\n",
    "    \n",
    "    pos_indices = []\n",
    "    \n",
    "    neg_indices = []\n",
    "    \n",
    "    pos_ent_names = []\n",
    "    \n",
    "    neg_ent_names = []\n",
    "    \n",
    "    previous_iob = None\n",
    "    \n",
    "    for i in range(len(sent)):\n",
    "        word = sent[i][0]\n",
    "        iob = sent[i][1]\n",
    "        if for_evaluation:\n",
    "            neg = sent[i][2]\n",
    "        \n",
    "        if previous_iob == iob or i == 0:\n",
    "            current_list.append(word)\n",
    "            \n",
    "        else:\n",
    "            final_list.append(current_list)\n",
    "            current_list = [word]\n",
    "            if previous_iob.startswith(\"I\"):\n",
    "                entity_indices.append(list_counter)\n",
    "                if for_evaluation:\n",
    "                    if previous_neg == False:\n",
    "                        pos_indices.append(list_counter)\n",
    "                    else:\n",
    "                        neg_indices.append(list_counter)\n",
    "            list_counter += 1\n",
    "            \n",
    "        previous_iob = iob\n",
    "        \n",
    "        if for_evaluation:\n",
    "            previous_neg = neg\n",
    "            \n",
    "    if final_list == []:\n",
    "        final_list.append(current_list)\n",
    "        if sent[-1][1].startswith(\"I\"):\n",
    "            entity_indices.append(list_counter)\n",
    "            if for_evaluation:\n",
    "                if sent[-1][2] == False:\n",
    "                    pos_indices.append(list_counter)\n",
    "                else:\n",
    "                    neg_indices.append(list_counter)\n",
    "    elif current_list != [] and current_list != final_list[-1]:\n",
    "        final_list.append(current_list)\n",
    "        if sent[-1][1].startswith(\"I\"):\n",
    "            entity_indices.append(list_counter)\n",
    "            if for_evaluation:\n",
    "                if sent[-1][2] == False:\n",
    "                    pos_indices.append(list_counter)\n",
    "                else:\n",
    "                    neg_indices.append(list_counter)\n",
    "\n",
    "    if for_evaluation:\n",
    "        for ent_idx in pos_indices:\n",
    "            for i, phrase in enumerate(final_list):\n",
    "                if i == ent_idx:\n",
    "                    pos_ent_names.append(\" \".join(word for word in final_list[i]))\n",
    "\n",
    "        for ent_idx in neg_indices:\n",
    "            for i, phrase in enumerate(final_list):\n",
    "                if i == ent_idx:\n",
    "                    neg_ent_names.append(\" \".join(word for word in final_list[i]))\n",
    "\n",
    "        return pos_ent_names, neg_ent_names\n",
    "    \n",
    "    else:\n",
    "        return final_list, entity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "367b03ac-f9d4-4779-a606-9dc550785cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_gold_data(sent_list, output_style=\"tags\", word_of_interest=None):\n",
    "    \n",
    "    if word_of_interest:\n",
    "        active_list = []\n",
    "        for sent in sent_list:\n",
    "            for word, iob, tag in sent:\n",
    "                if word.lower() == word_of_interest:\n",
    "                    active_list.append(sent)\n",
    "                    break\n",
    "    \n",
    "    else:\n",
    "        active_list = sent_list\n",
    "    \n",
    "    if output_style == \"tags\":\n",
    "        outputs = []\n",
    "        for sent in active_list:\n",
    "            out = []\n",
    "            for word, iob, neg in sent:\n",
    "                out.append(neg)\n",
    "            outputs.append(out)\n",
    "        return outputs\n",
    "    \n",
    "    elif output_style == \"entities\":\n",
    "        pos_ents = []\n",
    "        neg_ents = []\n",
    "        \n",
    "        for sent in active_list:\n",
    "            sent_pos_ents, sent_neg_ents = transform_sentence(sent, for_evaluation=True)\n",
    "            pos_ents.append(sent_pos_ents)\n",
    "            neg_ents.append(sent_neg_ents)\n",
    "            \n",
    "        return pos_ents, neg_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85e438f-0640-42f1-98d6-f95f802348e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(preds, gold, input_style=\"tags\", verbose=False):\n",
    "    \n",
    "    total = 0\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    if input_style == \"tags\":\n",
    "        for i, pred in enumerate(preds):\n",
    "            printed = False\n",
    "            for j, tag in enumerate(pred):\n",
    "                total += 1\n",
    "                if tag == True and gold[i][j] == True:\n",
    "                    true_pos += 1\n",
    "                elif tag == True and gold[i][j] == False:\n",
    "                    false_pos += 1\n",
    "                    if verbose and printed == False:\n",
    "                        print()\n",
    "                        print(\"Sentence number:\", i)\n",
    "                        print()\n",
    "                        print(\"Predicted sequence:\", pred)\n",
    "                        print(\"Actual sequence:\", gold[i])\n",
    "                        printed = True\n",
    "                elif tag == False and gold[i][j] == True:\n",
    "                    false_neg += 1\n",
    "                    if verbose and printed == False:\n",
    "                        print()\n",
    "                        print(\"Sentence number:\", i)\n",
    "                        print()\n",
    "                        print(\"Predicted sequence:\", pred)\n",
    "                        print(\"Actual sequence:\", gold[i])\n",
    "                        printed = True\n",
    "                elif tag == False and gold[i][j] == False:\n",
    "                    true_neg += 1\n",
    "                \n",
    "    if input_style == \"entities\":\n",
    "        pred_pos_ents = preds[0]\n",
    "        pred_neg_ents = preds[1]\n",
    "        gold_pos_ents = gold[0]\n",
    "        gold_neg_ents = gold[1]\n",
    "        \n",
    "        total = len([pred for preds in pred_pos_ents for pred in preds]) + len([pred for preds in pred_neg_ents for pred in preds])\n",
    "        \n",
    "        for i in range(len(pred_pos_ents)):\n",
    "            false_positives = []\n",
    "            false_negatives = []\n",
    "            for ent in pred_pos_ents[i]:\n",
    "                if ent in gold_pos_ents[i]:\n",
    "                    true_neg += 1  # Correctly predicting a positive (i.e. non-negative) entity is a true negative since \"negated\" is treated as the positive class.\n",
    "                    gold_pos_ents[i].remove(ent)\n",
    "                else:\n",
    "                    false_neg += 1\n",
    "                    if verbose:\n",
    "                        false_negatives.append(ent)\n",
    "            \n",
    "            for ent in pred_neg_ents[i]:\n",
    "                if ent in gold_neg_ents[i]:\n",
    "                    true_pos += 1\n",
    "                    gold_neg_ents[i].remove(ent)\n",
    "                else:\n",
    "                    false_pos += 1\n",
    "                    if verbose:\n",
    "                        false_positives.append(ent)\n",
    "                            \n",
    "            if verbose:\n",
    "                if false_positives or false_negatives:\n",
    "                    print(\"Sentence number:\", i)\n",
    "                if false_positives:\n",
    "                    print(\"False positives:\", false_positives)\n",
    "                if false_negatives:\n",
    "                    print(\"Missed:\", false_negatives)\n",
    "    \n",
    "    print(\"true pos\", true_pos)\n",
    "    correct = true_pos + true_neg\n",
    "    accuracy = correct / total\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"Evaluation config:\", input_style)\n",
    "    print(\"Accuracy:\", \"{:.2%}\".format(accuracy))\n",
    "    print(\"Precision:\", '{:.2%}'.format(precision))\n",
    "    print(\"Recall:\", '{:.2%}'.format(recall))\n",
    "    print(\"F1:\", '{:.2%}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96088bf7-03fd-4531-ad9f-a17155661224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def get_spacy_tokens(transformed_sent, entity_indices):\n",
    "    \"\"\"Maps entity indices to indices in a spacy object.\"\"\"\n",
    "    \n",
    "    ent_2_spacy = {}\n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    doc = nlp(string_sent)\n",
    "    \n",
    "    spacy_special = [\"'\", '\"', \":\", \";\", \",\", \"?\", \"!\", \".\", \"n't\", \"'m\", \" \"]\n",
    "    \n",
    "    for ent_idx in entity_indices:\n",
    "        substring = \" \".join([item for item in transformed_sent[ent_idx]])\n",
    "        for char in spacy_special:\n",
    "            if substring.endswith(char):\n",
    "                substring = substring[:-len(char)]\n",
    "        \n",
    "        if str(doc).count(substring) == 1:\n",
    "            # end = str(doc).index(substring) + len(substring)\n",
    "            end = str(doc).index(substring) + len(substring)\n",
    "            if substring.endswith(\" \"):\n",
    "                substring = substring[:-1]\n",
    "            try:\n",
    "                start = end - len(substring.split()[-1])\n",
    "            except:\n",
    "                print(transformed_sent)\n",
    "                print(entity_indices)\n",
    "                print(end)\n",
    "                print(substring.split())\n",
    "#             print(str(doc)[start:end])\n",
    "            span = doc.char_span(start, end)\n",
    "            n = 0\n",
    "            while not span and n<4:\n",
    "                span = doc.char_span(start, end+n)\n",
    "                n += 1\n",
    "            while not span and n<4:\n",
    "                n = 0\n",
    "                span = doc.char_span(start-n, end)\n",
    "                n += 1\n",
    "            if not span:\n",
    "                print(transformed_sent)\n",
    "                print(entity_indices)\n",
    "                print(substring)\n",
    "            else:\n",
    "                span = span[0]\n",
    "            index = span.i\n",
    "\n",
    "        elif str(doc).count(substring) > 1:\n",
    "            best_guess_distance = 10000\n",
    "            original = len(\" \".join([item for sublist in transformed_sent[:ent_idx] for item in sublist])) + 1\n",
    "            for idx in [_.start() for _ in re.finditer(substring, string_sent)]:\n",
    "                if abs(idx - original) < best_guess_distance:\n",
    "                    best_guess_distance = abs(idx - original)\n",
    "                    best_guess = idx\n",
    "            end = best_guess + len(substring)\n",
    "            start = end - len(substring.split()[-1])\n",
    "            span = doc.char_span(start, end)[0]\n",
    "            index = span.i\n",
    "        \n",
    "        ent_2_spacy[ent_idx] = index\n",
    "        \n",
    "    return ent_2_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b36081a-51a0-4964-9ca3-6fdf60f680f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_one_sentence(sent, negators, output_style=\"tags\", word_of_interest=None, verbose=False):\n",
    "    \n",
    "    if len(sent[0]) == 3:\n",
    "        active_sent = []\n",
    "        for word, iob, neg in sent:\n",
    "            active_sent.append((word, iob))\n",
    "            \n",
    "    if word_of_interest == None:\n",
    "        found = True\n",
    "            \n",
    "    else:\n",
    "        found = False\n",
    "        \n",
    "    for word, iob in active_sent:\n",
    "        if word.lower() == word_of_interest:\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if found:\n",
    "        \n",
    "        transformed_sent, entity_indices = transform_sentence(sent)\n",
    "\n",
    "        spacy_mapping = get_spacy_tokens(transformed_sent, entity_indices)\n",
    "\n",
    "        if output_style == \"tags\":\n",
    "            out = [0] * len(sent)\n",
    "            for negator in negators:\n",
    "                neg_indices = negator(transformed_sent, spacy_mapping, output_style)\n",
    "                if verbose:\n",
    "                    for index in neg_indices:\n",
    "                        print(\"Index\", index, \"negated by\", negator.__name__)\n",
    "                for idx in neg_indices:\n",
    "                    out[idx] += 1\n",
    "\n",
    "            for i, tag in enumerate(out):\n",
    "                if tag == 0:\n",
    "                    out[i] = False\n",
    "                else:\n",
    "                    out[i] = True\n",
    "\n",
    "            return out\n",
    "\n",
    "        if output_style == \"entities\":\n",
    "            pos_ents = []\n",
    "            neg_ents = []\n",
    "            for negator in negators:\n",
    "                # Note that the below gives indices of entities in transformed_sent, not the indices themselves\n",
    "                negator_pos_ents, negator_neg_ents = negator(transformed_sent, spacy_mapping, output_style=output_style)\n",
    "                for ent_idx in negator_pos_ents:\n",
    "                    if ent_idx not in pos_ents and ent_idx not in neg_ents:\n",
    "                        pos_ents.append(ent_idx)\n",
    "                for ent_idx in negator_neg_ents:\n",
    "                    if ent_idx in pos_ents:\n",
    "                        pos_ents.remove(ent_idx)\n",
    "                        neg_ents.append(ent_idx)\n",
    "                    elif ent_idx not in neg_ents:\n",
    "                        neg_ents.append(ent_idx)\n",
    "\n",
    "            pos_ent_names = []\n",
    "            neg_ent_names = []\n",
    "\n",
    "            for ent_idx in pos_ents:\n",
    "                for i, phrase in enumerate(transformed_sent):\n",
    "                    if i == ent_idx:\n",
    "                        pos_ent_names.append(\" \".join(word for word in transformed_sent[i]))\n",
    "\n",
    "            for ent_idx in neg_ents:\n",
    "                for i, phrase in enumerate(transformed_sent):\n",
    "                    if i == ent_idx:\n",
    "                        neg_ent_names.append(\" \".join(word for word in transformed_sent[i]))\n",
    "\n",
    "            return pos_ent_names, neg_ent_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc2e01de-4a2a-4df7-b152-e3d7f867be0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_sent_list(sent_list, negators, output_style=\"tags\", word_of_interest=None):\n",
    "            \n",
    "    if output_style == \"tags\":\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        for sent in sent_list:\n",
    "            sent_preds = predict_one_sentence(sent, negators, output_style, word_of_interest)\n",
    "            if sent_preds is not None:\n",
    "                preds.append(predict_one_sentence(sent, negators, output_style, word_of_interest))\n",
    "            \n",
    "        return preds\n",
    "            \n",
    "    if output_style == \"entities\":\n",
    "        all_pos = []\n",
    "        all_neg = []\n",
    "        \n",
    "        for sent in sent_list:\n",
    "            preds = predict_one_sentence(sent, negators, output_style, word_of_interest)\n",
    "            if preds is not None:\n",
    "                pos_ents, neg_ents = preds\n",
    "                all_pos.append(pos_ents)\n",
    "                all_neg.append(neg_ents)\n",
    "            \n",
    "        return all_pos, all_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b25c889-c676-4897-993f-c6ee0b040fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_negator_output(transformed_sent, entity_indices, negated_indices, output_style):\n",
    "    \n",
    "    if output_style == \"tags\":\n",
    "        preds = []\n",
    "        k = 0\n",
    "        for i, sublist in enumerate(transformed_sent):\n",
    "            if i in negated_indices:\n",
    "                for j in range(k, k+len(sublist)):\n",
    "                    preds.append(j)\n",
    "            k += len(sublist)\n",
    "                    \n",
    "        return preds\n",
    "    \n",
    "    if output_style == \"entities\":\n",
    "        return list(set(entity_indices) - set(negated_indices)), negated_indices\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35c10bfe-6449-4c4f-960a-873912c377e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def instead(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "    \n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    \n",
    "    doc = nlp(string_sent)\n",
    "    \n",
    "    negated_indices = []\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        i = spacy_idx\n",
    "        root_hits = 0\n",
    "        negated = False\n",
    "        while root_hits != 2 and negated == False:  # i.e., while there is a head. In spacy, the main clause verb is its own head\n",
    "            i = doc[i].head.i\n",
    "            if i == doc[i].head.i:\n",
    "                root_hits += 1\n",
    "            if str(doc[i]) == \"of\" and str(doc[i-1]) == \"instead\":\n",
    "                negated = True\n",
    "                \n",
    "        if negated:\n",
    "            negated_indices.append(ent_idx)\n",
    "            \n",
    "        else:\n",
    "            for i in range(6):\n",
    "                if spacy_idx - i >= 0:\n",
    "                    if doc[spacy_idx-i].text == \"instead\":\n",
    "                        negated_indices.append(ent_idx)\n",
    "                                                        \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cdf88d3-cb8b-488a-b1ee-d097073bb794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def no(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    \n",
    "    doc = nlp(string_sent)\n",
    "    \n",
    "    negated_indices = []\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        i = spacy_idx\n",
    "        negated = False\n",
    "        for j in range(3):\n",
    "            if i - j >= 0:\n",
    "                if doc[i-j].text.lower() == \"no\":\n",
    "                    negated = True\n",
    "                if doc[i-j].pos_ not in [\"ADJ\", \"NOUN\"]:\n",
    "                    break\n",
    "        if negated:\n",
    "            negated_indices.append(ent_idx)\n",
    "                                                        \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee00788-25f2-4314-855c-d4f76b511365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def more_less_than(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "    \n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "        \n",
    "    negated_indices = []\n",
    "    \n",
    "    if \"more than\" not in string_sent.lower() and \"less than\" not in string_sent.lower():\n",
    "        return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        i = spacy_idx\n",
    "        negated = False\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if \"more than\" in \" \".join(transformed_sent[ent_idx-1]).lower() or \"less than\" in \" \".join(transformed_sent[ent_idx-1]).lower():\n",
    "            negated = True\n",
    "        if negated:\n",
    "            negated_indices.append(ent_idx)\n",
    "                                                        \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cafad4b-b655-467b-94af-bf9a6cf20c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def without(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "    \n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    \n",
    "    doc = nlp(string_sent)\n",
    "        \n",
    "    negated_indices = []\n",
    "        \n",
    "    prev_negated = False\n",
    "    \n",
    "    if \"without\" not in string_sent.lower():\n",
    "        return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        \n",
    "        i = spacy_idx\n",
    "        negated = False\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        if \"without\" in \" \".join(transformed_sent[ent_idx-1]).lower() or prev_negated:\n",
    "            negated = True\n",
    "            if len(doc) > i+1:\n",
    "                if doc[i+1].pos_ == \"CCONJ\":\n",
    "                    prev_negated = True\n",
    "                else:\n",
    "                    prev_negated = False\n",
    "                    \n",
    "        if negated:\n",
    "            negated_indices.append(ent_idx)\n",
    "                                                        \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44124f5a-0e6a-489d-9113-09d250af40d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def not_(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "        \n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    \n",
    "    doc = nlp(string_sent)\n",
    "        \n",
    "    negated_indices = []\n",
    "        \n",
    "    entity_indices = sorted(spacy_mapping)\n",
    "    \n",
    "    end_punct = [\".\", \"!\", \"?\"]\n",
    "    \n",
    "    nots = {\"not\", \"n't\", \"no\", \"don't\", \"doesn't\", \"aren't\", \"isn't\"}\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        \n",
    "        if ent_idx in negated_indices:\n",
    "            continue\n",
    "        \n",
    "        i = spacy_idx\n",
    "            \n",
    "        if ent_idx == 0 or spacy_idx == 0:\n",
    "            continue\n",
    "        \n",
    "        negated = False\n",
    "        \n",
    "        negator = False\n",
    "        \n",
    "        if i == 0:\n",
    "            continue\n",
    "            \n",
    "        if len(nots - set(\" \".join(transformed_sent[ent_idx-1]).lower().split())) != len(nots):\n",
    "            if \".\" not in \" \".join(transformed_sent[ent_idx-1]) and \\\n",
    "            \"!\" not in \" \".join(transformed_sent[ent_idx-1]) and \\\n",
    "            \"?\" not in \" \".join(transformed_sent[ent_idx-1]):\n",
    "                negator = True\n",
    "            else:\n",
    "                occurrences = [k for k, n in enumerate(transformed_sent[ent_idx-1]) if n[-1] in end_punct]\n",
    "                if occurrences:\n",
    "                    if len(nots - set(transformed_sent[ent_idx-1][occurrences[-1]:])) != len(nots):\n",
    "                        negator = True\n",
    "                \n",
    "        elif not negated and len(nots - set(\" \".join(transformed_sent[ent_idx]).lower().split())) != len(nots):\n",
    "            for negator in nots:\n",
    "                if negator in transformed_sent[ent_idx] and transformed_sent[ent_idx].index(negator) < i:\n",
    "                    negator = True\n",
    "                    break\n",
    "                    \n",
    "        if negator and \"see\" not in transformed_sent[ent_idx-1]:\n",
    "            negated = True\n",
    "            negated_indices.append(ent_idx)\n",
    "            if i != len(doc)-1 and ent_idx != entity_indices[-1]:\n",
    "                j = i+1\n",
    "                ent_idx_idx = entity_indices.index(ent_idx)+1\n",
    "                while j - i < 10 and j < len(doc) and ent_idx_idx < len(entity_indices):\n",
    "                    if doc[j].lemma_ not in [\"need\", \"want\"] and doc[j].pos_ not in [\"CCONJ\", \"ADJ\", \"NOUN\", \"DET\", \"PRON\"]:\n",
    "                        break\n",
    "                    elif doc[j].text in transformed_sent[entity_indices[ent_idx_idx]][0]:\n",
    "                        if entity_indices[ent_idx_idx] not in negated_indices:\n",
    "                            negated_indices.append(entity_indices[ent_idx_idx])\n",
    "                        ent_idx_idx += 1\n",
    "                    j += 1\n",
    "                 \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab90c6f4-8809-420d-9367-50b4973c2dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verbs(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "    \n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    \n",
    "    doc = nlp(string_sent)\n",
    "    \n",
    "    negated_indices = []\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        i = spacy_idx\n",
    "        root_hits = 0\n",
    "        negated = False\n",
    "        while root_hits != 2 and negated == False and doc[i].head.pos_ != \"ADP\":  # i.e., while there is a head. In spacy, the main clause verb is its own head\n",
    "            i = doc[i].head.i\n",
    "            if i == doc[i].head.i:\n",
    "                root_hits += 1\n",
    "            if doc[i].lemma_ in [\"remove\", \"replace\"]:\n",
    "                negated = True\n",
    "                \n",
    "        if negated:\n",
    "            negated_indices.append(ent_idx)\n",
    "            \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b21ec28b-f8a2-44a8-ac25-3cc4f2c61e50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def comparative(transformed_sent, spacy_mapping, output_style=\"tags\"):\n",
    "    \n",
    "    string_sent = \" \".join([item for sublist in transformed_sent for item in sublist])\n",
    "    \n",
    "    doc = nlp(string_sent)\n",
    "        \n",
    "    negated_indices = []\n",
    "    \n",
    "    comparative = False\n",
    "    \n",
    "    for ent_idx, spacy_idx in spacy_mapping.items():\n",
    "        \n",
    "        i = spacy_idx\n",
    "        negated = False\n",
    "        \n",
    "        if comparative:\n",
    "            negated = True\n",
    "            \n",
    "        comparative = False\n",
    "        \n",
    "        if doc[i].pos_ == \"ADJ\":\n",
    "            if doc[i].text.endswith(\"er\") and not doc[i].lemma_.endswith(\"er\") and nlp(doc[i].lemma_)[0].pos_ == \"ADJ\":\n",
    "                if len(doc) > i+1 and doc[i+1].text == \"than\":\n",
    "                    comparative = True\n",
    "                \n",
    "        if negated:\n",
    "            negated_indices.append(ent_idx)\n",
    "        \n",
    "    return get_negator_output(transformed_sent, spacy_mapping.keys(), negated_indices, output_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "866542fc-fa27-44bb-a1f3-3605212049df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_and_evaluate(sent_list, negators, output_type=\"tags\", word_of_interest=None, verbose=False):\n",
    "\n",
    "    preds = predict_sent_list(sent_list, negators, output_type, word_of_interest)\n",
    "    gold = prepare_gold_data(sent_list, output_type, word_of_interest)\n",
    "    \n",
    "    evaluate(preds, gold, output_type, verbose)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f02bf36-ed64-4517-b26c-84847096cd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negators = [instead, more_less_than, without, not_, comparative, verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c421479-4464-4a9a-8493-d45a70b166a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number: 6\n",
      "Missed: ['scratchy.']\n",
      "Sentence number: 10\n",
      "False positives: ['alcove', 'shower walls?']\n",
      "Sentence number: 11\n",
      "False positives: ['bathtub']\n",
      "Sentence number: 15\n",
      "False positives: ['American standard']\n",
      "Sentence number: 16\n",
      "False positives: ['navy.']\n",
      "Missed: ['gold', 'black']\n",
      "Sentence number: 33\n",
      "False positives: ['screw']\n",
      "Sentence number: 42\n",
      "Missed: ['one hole']\n",
      "Sentence number: 71\n",
      "Missed: ['Bone', 'buscuit']\n",
      "Sentence number: 95\n",
      "Missed: ['size XS']\n",
      "Sentence number: 101\n",
      "False positives: ['screw less']\n",
      "Sentence number: 103\n",
      "False positives: ['72\" wide', 'vanities?']\n",
      "Sentence number: 119\n",
      "False positives: ['sink must have no holes. Faucet in on wall']\n",
      "Sentence number: 128\n",
      "False positives: ['29\"x29\"']\n",
      "Sentence number: 131\n",
      "False positives: ['50 inch wide 39 inch tall']\n",
      "Sentence number: 143\n",
      "False positives: ['good pressure']\n",
      "Sentence number: 145\n",
      "False positives: ['23\" wide.']\n",
      "true pos 92\n",
      "Evaluation config: entities\n",
      "Accuracy: 96.25%\n",
      "Precision: 86.79%\n",
      "Recall: 92.93%\n",
      "F1: 89.76%\n"
     ]
    }
   ],
   "source": [
    "predict_and_evaluate(all_sents, negators, \"entities\", None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e4213ab-48c8-404b-b1cc-bb19117c4efe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence number: 6\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 10\n",
      "\n",
      "Predicted sequence: [False, False, False, False, True, True, True]\n",
      "Actual sequence: [False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 11\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 15\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, True, False, False, False, False, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 16\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 33\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, True, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 42\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 71\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Actual sequence: [False, False, False, False, False, True, False, True, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 95\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 101\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 103\n",
      "\n",
      "Predicted sequence: [False, False, False, False, True, True, True]\n",
      "Actual sequence: [False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 119\n",
      "\n",
      "Predicted sequence: [False, False, False, False, True, True, True, True, True, True, True, True, True, False]\n",
      "Actual sequence: [False, False, False, False, True, True, True, True, True, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 128\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, True]\n",
      "Actual sequence: [False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 131\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 143\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "\n",
      "Sentence number: 145\n",
      "\n",
      "Predicted sequence: [False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False]\n",
      "Actual sequence: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "true pos 133\n",
      "Evaluation config: tags\n",
      "Accuracy: 98.42%\n",
      "Precision: 82.61%\n",
      "Recall: 93.66%\n",
      "F1: 87.79%\n"
     ]
    }
   ],
   "source": [
    "predict_and_evaluate(all_sents, negators, \"tags\", None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ef7a5c-da3a-4a81-bf50-7242afee89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_woi(sent_list, woi):\n",
    "    out = []\n",
    "    for sent in sent_list:\n",
    "        for word, iob, neg in sent:\n",
    "            if word == woi:\n",
    "                out.append(sent)\n",
    "                break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262cc68-9633-4f9e-800f-af78a22a7de7",
   "metadata": {},
   "source": [
    "### Reasons why scope detection is not always successful (based on partial sample of sentences):\n",
    "- No negator:\n",
    "  - instead (3)\n",
    "  - without (4)\n",
    "  - comparative (1) (e.g. \"deeper than a regular-True tub\")\n",
    "  - negation scope would include entities we don't want to be negated (2) (e.g., \"counter top with no drill-True holes-True for the faucet-False\")\n",
    "  - can X be removed (1)\n",
    "  - negative affix (1)\n",
    "- Miscellaneous:\n",
    "  - \"I would like to know if the wool-like top side is soft as well, or if it is scratchy-True\"\n",
    "  - \"You have it for the gold-True and black-True but i don't [sic] want it for the navy\"\n",
    "  - \"I found a vanity top I love but it has only one-True hole-True for taps and that doesn't suit our needs\"\n",
    " \n",
    " \n",
    "Not sure:\n",
    "- I'm looking for X, not Y or Z-True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a533835-b639-46d1-8fcc-15573e4d950a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ways of indicating negation (incomplete):\n",
    "\n",
    "- \"don't want (any)\"\n",
    "- \"no\"\n",
    "- \"no more than\"\n",
    "- \"less than\"\n",
    "- \"un-\"\n",
    "- \"without\"\n",
    "- \"only\"\n",
    "- \"-less\"\n",
    "- \"instead of\"\n",
    "- \"too\" (what comes before is negated, e.g. in \"pink is too light\", \"pink\" is negated)\n",
    "- \"not too\" (what comes after is negated, e.g. in \"not too light\", \"light\" is negated)\n",
    "- \"the website only gives the option of\"\n",
    "- \"without X or Y\" (two separately annotated entities separated by conjunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d882b-ef9a-4be8-a76b-969dc8da27c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
