{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-07T02:55:24.649868Z","iopub.execute_input":"2022-06-07T02:55:24.650256Z","iopub.status.idle":"2022-06-07T02:55:24.688505Z","shell.execute_reply.started":"2022-06-07T02:55:24.650180Z","shell.execute_reply":"2022-06-07T02:55:24.687689Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nertag/SOCC_negation.tsv\n/kaggle/input/nertag/clean_data.csv\n/kaggle/input/nertag/unnegated_File_Name.csv\n/kaggle/input/nertag/un-negated_clean_data.csv\n/kaggle/input/nertag/File_Name.csv\n/kaggle/input/nertag/reannotated_val.csv\n/kaggle/input/nertag/Heyday_validationdata.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Changing the transformers and tokenizer versions","metadata":{}},{"cell_type":"code","source":"pip install tokenizers==0.8.0rc4","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:55:36.569656Z","iopub.execute_input":"2022-06-07T02:55:36.570518Z","iopub.status.idle":"2022-06-07T02:55:50.538177Z","shell.execute_reply.started":"2022-06-07T02:55:36.570457Z","shell.execute_reply":"2022-06-07T02:55:50.537231Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tokenizers==0.8.0rc4\n  Downloading tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 4.18.0 requires tokenizers!=0.11.3,<0.13,>=0.11.1, but you have tokenizers 0.8.0rc4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.8.0rc4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers==3.0.1","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:55:52.073162Z","iopub.execute_input":"2022-06-07T02:55:52.074077Z","iopub.status.idle":"2022-06-07T02:56:07.781482Z","shell.execute_reply.started":"2022-06-07T02:55:52.074039Z","shell.execute_reply":"2022-06-07T02:56:07.780601Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting transformers==3.0.1\n  Downloading transformers-3.0.1-py3-none-any.whl (757 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.2/757.2 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (21.3)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (0.1.96)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (4.63.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (2.27.1)\nRequirement already satisfied: tokenizers==0.8.0-rc4 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (0.8.0rc4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (3.6.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.1) (0.0.53)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.1) (3.0.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (1.26.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.1) (2021.10.8)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.1) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.1) (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.1) (1.16.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses->transformers==3.0.1) (4.11.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==3.0.1) (3.7.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==3.0.1) (4.2.0)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.18.0\n    Uninstalling transformers-4.18.0:\n      Successfully uninstalled transformers-4.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.9.3 requires transformers<4.19,>=4.1, but you have transformers 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed transformers-3.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\ntransformers.__version__","metadata":{"execution":{"iopub.status.busy":"2022-06-06T06:23:41.033693Z","iopub.execute_input":"2022-06-06T06:23:41.033954Z","iopub.status.idle":"2022-06-06T06:23:49.565417Z","shell.execute_reply.started":"2022-06-06T06:23:41.033919Z","shell.execute_reply":"2022-06-06T06:23:49.564635Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\nThe current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"The current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'3.0.1'"},"metadata":{}}]},{"cell_type":"code","source":"import tokenizers\ntokenizers.__version__","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:07.786662Z","iopub.execute_input":"2022-06-07T02:56:07.786955Z","iopub.status.idle":"2022-06-07T02:56:07.821886Z","shell.execute_reply.started":"2022-06-07T02:56:07.786914Z","shell.execute_reply":"2022-06-07T02:56:07.821100Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'0.8.0.rc4'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification,AutoConfig","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:07.826232Z","iopub.execute_input":"2022-06-07T02:56:07.828359Z","iopub.status.idle":"2022-06-07T02:56:17.628283Z","shell.execute_reply.started":"2022-06-07T02:56:07.828309Z","shell.execute_reply":"2022-06-07T02:56:17.627178Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\nThe current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"The current process just got forked. Disabling parallelism to avoid deadlocks...\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Reading the SOCC data","metadata":{}},{"cell_type":"code","source":"negation_data_socc= pd.read_csv(\"../input/nertag/SOCC_negation.tsv\", sep = \"\\t\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:17.630662Z","iopub.execute_input":"2022-06-07T02:56:17.631178Z","iopub.status.idle":"2022-06-07T02:56:17.711769Z","shell.execute_reply.started":"2022-06-07T02:56:17.631131Z","shell.execute_reply":"2022-06-07T02:56:17.710929Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"negation_data_socc","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:21.477925Z","iopub.execute_input":"2022-06-07T02:56:21.478564Z","iopub.status.idle":"2022-06-07T02:56:21.503126Z","shell.execute_reply.started":"2022-06-07T02:56:21.478524Z","shell.execute_reply":"2022-06-07T02:56:21.502299Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       Sentence_index  Token_index   Token  Negation\n0                   0            1  Europe     False\n1                   0            2     has     False\n2                   0            3    done     False\n3                   0            4       a     False\n4                   0            5    HUGE     False\n...               ...          ...     ...       ...\n54883            2595           48       ,     False\n54884            2595           49    none     False\n54885            2595           50       ,     False\n54886            2595           51       ,     False\n54887            2595           52       ,     False\n\n[54888 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence_index</th>\n      <th>Token_index</th>\n      <th>Token</th>\n      <th>Negation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>Europe</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>has</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>3</td>\n      <td>done</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>4</td>\n      <td>a</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>5</td>\n      <td>HUGE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54883</th>\n      <td>2595</td>\n      <td>48</td>\n      <td>,</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>54884</th>\n      <td>2595</td>\n      <td>49</td>\n      <td>none</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>54885</th>\n      <td>2595</td>\n      <td>50</td>\n      <td>,</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>54886</th>\n      <td>2595</td>\n      <td>51</td>\n      <td>,</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>54887</th>\n      <td>2595</td>\n      <td>52</td>\n      <td>,</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>54888 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"negation_data_socc.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:25.319733Z","iopub.execute_input":"2022-06-07T02:56:25.320033Z","iopub.status.idle":"2022-06-07T02:56:25.354534Z","shell.execute_reply.started":"2022-06-07T02:56:25.319995Z","shell.execute_reply":"2022-06-07T02:56:25.353636Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 54888 entries, 0 to 54887\nData columns (total 4 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   Sentence_index  54888 non-null  int64 \n 1   Token_index     54888 non-null  int64 \n 2   Token           54888 non-null  object\n 3   Negation        54888 non-null  bool  \ndtypes: bool(1), int64(2), object(1)\nmemory usage: 1.3+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Wrangling","metadata":{}},{"cell_type":"code","source":"negation_data_socc['Negation'] = negation_data_socc['Negation'].map({True: 'True', False: 'False'})","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:27.934898Z","iopub.execute_input":"2022-06-07T02:56:27.935207Z","iopub.status.idle":"2022-06-07T02:56:27.951301Z","shell.execute_reply.started":"2022-06-07T02:56:27.935170Z","shell.execute_reply":"2022-06-07T02:56:27.950334Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"negation_data_socc['Sentence']= negation_data_socc[['Sentence_index','Token','Negation']].groupby(['Sentence_index'])['Token'].transform(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:30.851908Z","iopub.execute_input":"2022-06-07T02:56:30.852662Z","iopub.status.idle":"2022-06-07T02:56:31.102474Z","shell.execute_reply.started":"2022-06-07T02:56:30.852620Z","shell.execute_reply":"2022-06-07T02:56:31.101600Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"negation_data_socc['Negation']= negation_data_socc[['Sentence_index','Token','Negation']].groupby(['Sentence_index'])['Negation'].transform(lambda x: ','.join(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:32.646379Z","iopub.execute_input":"2022-06-07T02:56:32.646654Z","iopub.status.idle":"2022-06-07T02:56:32.886669Z","shell.execute_reply.started":"2022-06-07T02:56:32.646625Z","shell.execute_reply":"2022-06-07T02:56:32.885855Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"negation_data_socc = negation_data_socc[['Sentence','Negation']]","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:35.322814Z","iopub.execute_input":"2022-06-07T02:56:35.323074Z","iopub.status.idle":"2022-06-07T02:56:35.329385Z","shell.execute_reply.started":"2022-06-07T02:56:35.323044Z","shell.execute_reply":"2022-06-07T02:56:35.328564Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"negation_data_socc = negation_data_socc.drop_duplicates().reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:38.772108Z","iopub.execute_input":"2022-06-07T02:56:38.772873Z","iopub.status.idle":"2022-06-07T02:56:38.818731Z","shell.execute_reply.started":"2022-06-07T02:56:38.772824Z","shell.execute_reply":"2022-06-07T02:56:38.817909Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"negation_data_socc","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:43.208480Z","iopub.execute_input":"2022-06-07T02:56:43.208952Z","iopub.status.idle":"2022-06-07T02:56:43.224623Z","shell.execute_reply.started":"2022-06-07T02:56:43.208917Z","shell.execute_reply":"2022-06-07T02:56:43.223596Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                               Sentence  \\\n0     Europe has done a HUGE amount to help refugees...   \n1                     But that is not going to happen !   \n2         Hilary did not lose because she was a woman .   \n3     She lost because the US had their version of a...   \n4            A restless electorate wanted an outsider .   \n...                                                 ...   \n2573  We are going to tell them that plenty of other...   \n2574  Some of them even ' third world countries ' . ...   \n2575  We are going to tell them , when they run for ...   \n2576  Or the job.And we are going to tell our sons t...   \n2577  OMG , , , someone who has understanding , , , ...   \n\n                                               Negation  \n0     False,False,False,False,False,False,False,Fals...  \n1          False,False,False,False,True,True,True,False  \n2     False,False,False,True,True,True,True,True,Tru...  \n3     False,False,False,False,False,False,False,Fals...  \n4             False,False,False,False,False,False,False  \n...                                                 ...  \n2573  False,False,False,False,False,False,False,Fals...  \n2574  False,False,False,False,False,False,False,Fals...  \n2575  False,False,False,False,False,False,False,Fals...  \n2576  False,False,False,False,False,False,False,Fals...  \n2577  False,False,False,False,False,False,False,Fals...  \n\n[2578 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Negation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Europe has done a HUGE amount to help refugees...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>But that is not going to happen !</td>\n      <td>False,False,False,False,True,True,True,False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hilary did not lose because she was a woman .</td>\n      <td>False,False,False,True,True,True,True,True,Tru...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>She lost because the US had their version of a...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A restless electorate wanted an outsider .</td>\n      <td>False,False,False,False,False,False,False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2573</th>\n      <td>We are going to tell them that plenty of other...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>2574</th>\n      <td>Some of them even ' third world countries ' . ...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>2575</th>\n      <td>We are going to tell them , when they run for ...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>2576</th>\n      <td>Or the job.And we are going to tell our sons t...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>2577</th>\n      <td>OMG , , , someone who has understanding , , , ...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2578 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"labels_to_ids = {'True':1,'False':0}\nids_to_labels = {1: 'True', 0: 'False'}","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:47.164148Z","iopub.execute_input":"2022-06-07T02:56:47.164587Z","iopub.status.idle":"2022-06-07T02:56:47.171314Z","shell.execute_reply.started":"2022-06-07T02:56:47.164546Z","shell.execute_reply":"2022-06-07T02:56:47.170442Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 1\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:56:58.854691Z","iopub.execute_input":"2022-06-07T02:56:58.854980Z","iopub.status.idle":"2022-06-07T02:56:59.786596Z","shell.execute_reply.started":"2022-06-07T02:56:58.854946Z","shell.execute_reply":"2022-06-07T02:56:59.785854Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baac424462db40a99ad67854ecc9438a"}},"metadata":{}}]},{"cell_type":"code","source":"class dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n  def __getitem__(self, index):\n        # step 1: get the sentence and word labels \n        sentence = self.data.Sentence[index].strip().split()  \n        word_labels = self.data.Negation[index].split(\",\") \n\n        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n        encoding = self.tokenizer(sentence,\n                             is_pretokenized=True, \n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        # step 3: create token labels only for first word pieces of each tokenized word\n        labels = [labels_to_ids[label] for label in word_labels] \n        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n        # create an empty array of -100 of length max_length\n        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n        \n        # set only labels whose first offset position is 0 and the second is not 0\n        i = 0\n        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n          if mapping[0] == 0 and mapping[1] != 0:\n            # overwrite label\n            encoded_labels[idx] = labels[i]\n            i += 1\n\n        # step 4: turn everything into PyTorch tensors\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        item['labels'] = torch.as_tensor(encoded_labels)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:03.659623Z","iopub.execute_input":"2022-06-07T02:57:03.659916Z","iopub.status.idle":"2022-06-07T02:57:03.671138Z","shell.execute_reply.started":"2022-06-07T02:57:03.659883Z","shell.execute_reply":"2022-06-07T02:57:03.670079Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_size = 0.8\ntrain_dataset = negation_data_socc.sample(frac=train_size,random_state=200)\ntest_dataset = negation_data_socc.drop(train_dataset.index).reset_index(drop=True)\ntest_dataset.drop(test_dataset.index[[233, 234, 457]], inplace=True)\ntest_dataset = test_dataset.reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\ntrain_dataset.drop(train_dataset.index[[573, 1506, 1603, 1700]], inplace=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(negation_data_socc.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = dataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = dataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:08.247877Z","iopub.execute_input":"2022-06-07T02:57:08.248194Z","iopub.status.idle":"2022-06-07T02:57:08.265929Z","shell.execute_reply.started":"2022-06-07T02:57:08.248157Z","shell.execute_reply":"2022-06-07T02:57:08.264911Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"FULL Dataset: (2578, 2)\nTRAIN Dataset: (2058, 2)\nTEST Dataset: (513, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing and wrangling the HeyDay validation set","metadata":{}},{"cell_type":"code","source":"external_valset= pd.read_csv(\"../input/nertag/un-negated_clean_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:12.356008Z","iopub.execute_input":"2022-06-07T02:57:12.356759Z","iopub.status.idle":"2022-06-07T02:57:12.370004Z","shell.execute_reply.started":"2022-06-07T02:57:12.356719Z","shell.execute_reply":"2022-06-07T02:57:12.368945Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"external_valset","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:15.820465Z","iopub.execute_input":"2022-06-07T02:57:15.821046Z","iopub.status.idle":"2022-06-07T02:57:15.836037Z","shell.execute_reply.started":"2022-06-07T02:57:15.821005Z","shell.execute_reply":"2022-06-07T02:57:15.835262Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"       Tokens       Tags  is_negative  sentence\n0           I          O        False         0\n1          am          O        False         0\n2     looking          O        False         0\n3         for          O        False         0\n4           a          O        False         0\n...       ...        ...          ...       ...\n2345       30     B-SIZE        False       166\n2346     inch     I-SIZE        False       166\n2347    white   B-COLOUR        False       166\n2348     desk  B-PRODUCT        False       166\n2349      set  I-PRODUCT        False       166\n\n[2350 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Tags</th>\n      <th>is_negative</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I</td>\n      <td>O</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>am</td>\n      <td>O</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>looking</td>\n      <td>O</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>for</td>\n      <td>O</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>O</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>30</td>\n      <td>B-SIZE</td>\n      <td>False</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2346</th>\n      <td>inch</td>\n      <td>I-SIZE</td>\n      <td>False</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>white</td>\n      <td>B-COLOUR</td>\n      <td>False</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>desk</td>\n      <td>B-PRODUCT</td>\n      <td>False</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>set</td>\n      <td>I-PRODUCT</td>\n      <td>False</td>\n      <td>166</td>\n    </tr>\n  </tbody>\n</table>\n<p>2350 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"external_valset[\"Sentence\"]= external_valset[[\"Tokens\",\"is_negative\",\"sentence\"]].groupby([\"sentence\"])[\"Tokens\"].transform(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:19.071678Z","iopub.execute_input":"2022-06-07T02:57:19.072072Z","iopub.status.idle":"2022-06-07T02:57:19.150661Z","shell.execute_reply.started":"2022-06-07T02:57:19.072014Z","shell.execute_reply":"2022-06-07T02:57:19.148584Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"external_valset['is_negative'] = external_valset['is_negative'].map({True: 'True', False: 'False'})","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:21.454591Z","iopub.execute_input":"2022-06-07T02:57:21.455380Z","iopub.status.idle":"2022-06-07T02:57:21.461203Z","shell.execute_reply.started":"2022-06-07T02:57:21.455317Z","shell.execute_reply":"2022-06-07T02:57:21.460356Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"external_valset[\"Negation\"]= external_valset[[\"Tokens\",\"is_negative\",\"sentence\"]].groupby([\"sentence\"])[\"is_negative\"].transform(lambda x: ','.join(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:23.706189Z","iopub.execute_input":"2022-06-07T02:57:23.706474Z","iopub.status.idle":"2022-06-07T02:57:23.735785Z","shell.execute_reply.started":"2022-06-07T02:57:23.706442Z","shell.execute_reply":"2022-06-07T02:57:23.735020Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"external_valset = external_valset[['Sentence','Negation']]","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:26.245383Z","iopub.execute_input":"2022-06-07T02:57:26.246157Z","iopub.status.idle":"2022-06-07T02:57:26.254054Z","shell.execute_reply.started":"2022-06-07T02:57:26.246110Z","shell.execute_reply":"2022-06-07T02:57:26.253139Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"external_valset = external_valset.drop_duplicates().reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:28.889862Z","iopub.execute_input":"2022-06-07T02:57:28.890175Z","iopub.status.idle":"2022-06-07T02:57:28.897812Z","shell.execute_reply.started":"2022-06-07T02:57:28.890139Z","shell.execute_reply":"2022-06-07T02:57:28.897025Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"external_valset","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:31.892640Z","iopub.execute_input":"2022-06-07T02:57:31.893439Z","iopub.status.idle":"2022-06-07T02:57:31.905870Z","shell.execute_reply.started":"2022-06-07T02:57:31.893375Z","shell.execute_reply":"2022-06-07T02:57:31.905120Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                              Sentence  \\\n0    I am looking for a black gloss 33 inch firecla...   \n1    Looking for pre workout Pump addict instead of...   \n2    i need a 48 inch glass sliding goof and a show...   \n3    Hello, do any of your free standing tubs have ...   \n4    I'm looking for a 24 inch white mirror that is...   \n..                                                 ...   \n162       What rectangular shower units are available?   \n163  I am looking for a 35 inch bathroom sink count...   \n164  I'm looking f or a Black Matte bathtub double ...   \n165               Need a 27 inch frameless shower door   \n166                 Looking for 30 inch white desk set   \n\n                                              Negation  \n0    False,False,False,False,False,False,False,Fals...  \n1    False,False,False,False,False,False,False,Fals...  \n2    False,False,False,False,False,False,False,Fals...  \n3    False,False,False,False,False,False,False,Fals...  \n4    False,False,False,False,False,False,False,Fals...  \n..                                                 ...  \n162                False,False,False,False,False,False  \n163  False,False,False,False,False,False,False,Fals...  \n164  False,False,False,False,False,False,False,Fals...  \n165          False,False,False,False,False,False,False  \n166          False,False,False,False,False,False,False  \n\n[167 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Negation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I am looking for a black gloss 33 inch firecla...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Looking for pre workout Pump addict instead of...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i need a 48 inch glass sliding goof and a show...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hello, do any of your free standing tubs have ...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I'm looking for a 24 inch white mirror that is...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>What rectangular shower units are available?</td>\n      <td>False,False,False,False,False,False</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>I am looking for a 35 inch bathroom sink count...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>I'm looking f or a Black Matte bathtub double ...</td>\n      <td>False,False,False,False,False,False,False,Fals...</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>Need a 27 inch frameless shower door</td>\n      <td>False,False,False,False,False,False,False</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>Looking for 30 inch white desk set</td>\n      <td>False,False,False,False,False,False,False</td>\n    </tr>\n  </tbody>\n</table>\n<p>167 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_validation_set = dataset(external_valset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:35.867801Z","iopub.execute_input":"2022-06-07T02:57:35.868268Z","iopub.status.idle":"2022-06-07T02:57:35.873068Z","shell.execute_reply.started":"2022-06-07T02:57:35.868221Z","shell.execute_reply":"2022-06-07T02:57:35.872320Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Loading the BERT Model","metadata":{}},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:39.294414Z","iopub.execute_input":"2022-06-07T02:57:39.294806Z","iopub.status.idle":"2022-06-07T02:57:39.305335Z","shell.execute_reply.started":"2022-06-07T02:57:39.294757Z","shell.execute_reply":"2022-06-07T02:57:39.304457Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"valid_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\nvalid_loader = DataLoader(final_validation_set, **valid_params)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:42.059317Z","iopub.execute_input":"2022-06-07T02:57:42.059622Z","iopub.status.idle":"2022-06-07T02:57:42.064691Z","shell.execute_reply.started":"2022-06-07T02:57:42.059591Z","shell.execute_reply":"2022-06-07T02:57:42.063854Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:45.504873Z","iopub.execute_input":"2022-06-07T02:57:45.505194Z","iopub.status.idle":"2022-06-07T02:57:45.570280Z","shell.execute_reply.started":"2022-06-07T02:57:45.505155Z","shell.execute_reply":"2022-06-07T02:57:45.569402Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:57:48.974974Z","iopub.execute_input":"2022-06-07T02:57:48.975798Z","iopub.status.idle":"2022-06-07T02:58:20.162613Z","shell.execute_reply.started":"2022-06-07T02:57:48.975757Z","shell.execute_reply":"2022-06-07T02:58:20.160339Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699aeca498664f1f8e6add9892e0ea5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4603abf9f2cf4d81bbe4b826c5c1ea00"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"inputs = training_set[2]\ninput_ids = inputs[\"input_ids\"].unsqueeze(0)\nattention_mask = inputs[\"attention_mask\"].unsqueeze(0)\nlabels = inputs[\"labels\"].unsqueeze(0)\n\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\nlabels = labels.to(device)\n\noutputs = model(input_ids, attention_mask=attention_mask, labels=labels)\ninitial_loss = outputs[0]\ninitial_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:58:28.149735Z","iopub.execute_input":"2022-06-07T02:58:28.150023Z","iopub.status.idle":"2022-06-07T02:58:29.281405Z","shell.execute_reply.started":"2022-06-07T02:58:28.149988Z","shell.execute_reply":"2022-06-07T02:58:29.280570Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"tr_logits = outputs[1]\ntr_logits.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:58:31.991771Z","iopub.execute_input":"2022-06-07T02:58:31.992114Z","iopub.status.idle":"2022-06-07T02:58:31.998910Z","shell.execute_reply.started":"2022-06-07T02:58:31.992056Z","shell.execute_reply":"2022-06-07T02:58:31.997787Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128, 2])"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:58:34.156733Z","iopub.execute_input":"2022-06-07T02:58:34.157030Z","iopub.status.idle":"2022-06-07T02:58:34.164220Z","shell.execute_reply.started":"2022-06-07T02:58:34.156999Z","shell.execute_reply":"2022-06-07T02:58:34.163265Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Training and fine-tuneing the BERT model using the SOCC dataset","metadata":{}},{"cell_type":"code","source":"\ndef train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 100==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 100 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_labels.extend(labels)\n        tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:58:37.998868Z","iopub.execute_input":"2022-06-07T02:58:37.999181Z","iopub.status.idle":"2022-06-07T02:58:38.012566Z","shell.execute_reply.started":"2022-06-07T02:58:37.999146Z","shell.execute_reply":"2022-06-07T02:58:38.011627Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:58:43.063895Z","iopub.execute_input":"2022-06-07T02:58:43.064505Z","iopub.status.idle":"2022-06-07T02:59:28.722375Z","shell.execute_reply.started":"2022-06-07T02:58:43.064466Z","shell.execute_reply":"2022-06-07T02:59:28.721536Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Training epoch: 1\nTraining loss per 100 training steps: 0.8718448281288147\nTraining loss per 100 training steps: 0.42529961459412435\nTraining loss per 100 training steps: 0.3253485243234654\nTraining loss per 100 training steps: 0.2539540249382477\nTraining loss per 100 training steps: 0.21394646742341672\nTraining loss per 100 training steps: 0.19553941463147237\nTraining loss epoch: 0.19205539854880857\nTraining accuracy epoch: 0.9280739279681268\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['input_ids'].to(device, dtype = torch.long)\n            mask = batch['attention_mask'].to(device, dtype = torch.long)\n            labels = batch['labels'].to(device, dtype = torch.long)\n            \n            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n            \n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += labels.size(0)\n        \n            if idx % 100==0:\n                loss_step = eval_loss/nb_eval_steps\n                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # compute evaluation accuracy\n            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            \n            # only compute accuracy at active labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            \n            eval_labels.extend(labels)\n            eval_preds.extend(predictions)\n            \n            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n\n    labels = [ids_to_labels[id.item()] for id in eval_labels]\n    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:59:34.638948Z","iopub.execute_input":"2022-06-07T02:59:34.639413Z","iopub.status.idle":"2022-06-07T02:59:34.652897Z","shell.execute_reply.started":"2022-06-07T02:59:34.639368Z","shell.execute_reply":"2022-06-07T02:59:34.651618Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Validating the BERT model on the test split of the SOCC dataset","metadata":{}},{"cell_type":"code","source":"labels, predictions = valid(model, testing_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:59:37.488035Z","iopub.execute_input":"2022-06-07T02:59:37.488357Z","iopub.status.idle":"2022-06-07T02:59:41.369327Z","shell.execute_reply.started":"2022-06-07T02:59:37.488321Z","shell.execute_reply":"2022-06-07T02:59:41.367816Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Validation loss per 100 evaluation steps: 0.7898760437965393\nValidation loss per 100 evaluation steps: 0.07695304542966648\nValidation loss per 100 evaluation steps: 0.08311045853002924\nValidation Loss: 0.07939997079224119\nValidation Accuracy: 0.9742784034994237\n","output_type":"stream"}]},{"cell_type":"code","source":"# from seqeval.metrics import classification_report\nfrom sklearn.metrics import f1_score, classification_report\n\nprint(classification_report(labels, predictions, labels = list(labels_to_ids.keys()) ))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:59:49.133362Z","iopub.execute_input":"2022-06-07T02:59:49.133648Z","iopub.status.idle":"2022-06-07T02:59:49.299270Z","shell.execute_reply.started":"2022-06-07T02:59:49.133615Z","shell.execute_reply":"2022-06-07T02:59:49.298329Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        True       0.85      0.95      0.90      1402\n       False       0.99      0.97      0.98      9234\n\n    accuracy                           0.97     10636\n   macro avg       0.92      0.96      0.94     10636\nweighted avg       0.97      0.97      0.97     10636\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Validating the BERT model on the HeyDay Validation set.","metadata":{}},{"cell_type":"code","source":"labels, predictions = valid(model, valid_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T02:59:54.507314Z","iopub.execute_input":"2022-06-07T02:59:54.507668Z","iopub.status.idle":"2022-06-07T02:59:55.852436Z","shell.execute_reply.started":"2022-06-07T02:59:54.507627Z","shell.execute_reply":"2022-06-07T02:59:55.851424Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Validation loss per 100 evaluation steps: 0.44423457980155945\nValidation Loss: 0.3378222560346503\nValidation Accuracy: 0.8931383090178608\n","output_type":"stream"}]},{"cell_type":"code","source":"print(classification_report(labels, predictions, labels = list(labels_to_ids.keys()) ))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:00:00.849245Z","iopub.execute_input":"2022-06-07T03:00:00.849718Z","iopub.status.idle":"2022-06-07T03:00:00.895441Z","shell.execute_reply.started":"2022-06-07T03:00:00.849682Z","shell.execute_reply":"2022-06-07T03:00:00.894433Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        True       0.31      0.72      0.43       145\n       False       0.98      0.89      0.93      2205\n\n    accuracy                           0.88      2350\n   macro avg       0.64      0.81      0.68      2350\nweighted avg       0.94      0.88      0.90      2350\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Writing into the CSV file and generating the final tags","metadata":{}},{"cell_type":"code","source":"valset= pd.read_csv(\"../input/nertag/Heyday_validationdata.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:00:15.746063Z","iopub.execute_input":"2022-06-07T03:00:15.746654Z","iopub.status.idle":"2022-06-07T03:00:15.758697Z","shell.execute_reply.started":"2022-06-07T03:00:15.746617Z","shell.execute_reply":"2022-06-07T03:00:15.757730Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"valset","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:00:18.181638Z","iopub.execute_input":"2022-06-07T03:00:18.181921Z","iopub.status.idle":"2022-06-07T03:00:18.195461Z","shell.execute_reply.started":"2022-06-07T03:00:18.181888Z","shell.execute_reply":"2022-06-07T03:00:18.194548Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"      Sentence_Index   Tokens Predicted_NER_Tags\n0                  0        I                  O\n1                  0       am                  O\n2                  0  looking                  O\n3                  0      for                  O\n4                  0        a                  O\n...              ...      ...                ...\n2345             166       30             I-SIZE\n2346             166     inch             I-SIZE\n2347             166    white           I-COLOUR\n2348             166     desk          I-PRODUCT\n2349             166      set          I-PRODUCT\n\n[2350 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence_Index</th>\n      <th>Tokens</th>\n      <th>Predicted_NER_Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>am</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>looking</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>for</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>a</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>166</td>\n      <td>30</td>\n      <td>I-SIZE</td>\n    </tr>\n    <tr>\n      <th>2346</th>\n      <td>166</td>\n      <td>inch</td>\n      <td>I-SIZE</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>166</td>\n      <td>white</td>\n      <td>I-COLOUR</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>166</td>\n      <td>desk</td>\n      <td>I-PRODUCT</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>166</td>\n      <td>set</td>\n      <td>I-PRODUCT</td>\n    </tr>\n  </tbody>\n</table>\n<p>2350 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"token_list = valset.Tokens.values.tolist()\nsentence_list = valset.Sentence_Index.values.tolist()\nNER_list = valset.Predicted_NER_Tags.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:00:21.558555Z","iopub.execute_input":"2022-06-07T03:00:21.558836Z","iopub.status.idle":"2022-06-07T03:00:21.563648Z","shell.execute_reply.started":"2022-06-07T03:00:21.558804Z","shell.execute_reply":"2022-06-07T03:00:21.562861Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import csv\nrows = zip(sentence_list,token_list,NER_list,predictions)\nwith open('Heyday_predvalidationdata.csv', \"w\") as f:\n    writer = csv.writer(f)\n    writer.writerow(('Sentence_Index','Tokens','Predicted_NER_Tags','Predicted_Negation'))\n    for row in rows:\n        writer.writerow(row)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:00:26.476831Z","iopub.execute_input":"2022-06-07T03:00:26.477209Z","iopub.status.idle":"2022-06-07T03:00:26.491031Z","shell.execute_reply.started":"2022-06-07T03:00:26.477165Z","shell.execute_reply":"2022-06-07T03:00:26.490019Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"final_valset= pd.read_csv(\"../input/nertag/Heyday_final_predvalidationdata.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:46:45.226779Z","iopub.execute_input":"2022-06-07T03:46:45.227065Z","iopub.status.idle":"2022-06-07T03:46:45.241266Z","shell.execute_reply.started":"2022-06-07T03:46:45.227033Z","shell.execute_reply":"2022-06-07T03:46:45.240306Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"final_valset","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:46:48.001964Z","iopub.execute_input":"2022-06-07T03:46:48.002553Z","iopub.status.idle":"2022-06-07T03:46:48.015652Z","shell.execute_reply.started":"2022-06-07T03:46:48.002512Z","shell.execute_reply":"2022-06-07T03:46:48.014678Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"      Sentence_Index   Tokens Predicted_NER_Tags  Predicted_Negation\n0                  0        I                  O               False\n1                  0       am                  O               False\n2                  0  looking                  O               False\n3                  0      for                  O               False\n4                  0        a                  O               False\n...              ...      ...                ...                 ...\n2345             166       30             I-SIZE               False\n2346             166     inch             I-SIZE               False\n2347             166    white           I-COLOUR               False\n2348             166     desk          I-PRODUCT               False\n2349             166      set          I-PRODUCT               False\n\n[2350 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence_Index</th>\n      <th>Tokens</th>\n      <th>Predicted_NER_Tags</th>\n      <th>Predicted_Negation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I</td>\n      <td>O</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>am</td>\n      <td>O</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>looking</td>\n      <td>O</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>for</td>\n      <td>O</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>a</td>\n      <td>O</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>166</td>\n      <td>30</td>\n      <td>I-SIZE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2346</th>\n      <td>166</td>\n      <td>inch</td>\n      <td>I-SIZE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>166</td>\n      <td>white</td>\n      <td>I-COLOUR</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>166</td>\n      <td>desk</td>\n      <td>I-PRODUCT</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>166</td>\n      <td>set</td>\n      <td>I-PRODUCT</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>2350 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_valset = final_valset.replace('I-','', regex=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:47:20.148478Z","iopub.execute_input":"2022-06-07T03:47:20.148739Z","iopub.status.idle":"2022-06-07T03:47:20.163115Z","shell.execute_reply.started":"2022-06-07T03:47:20.148709Z","shell.execute_reply":"2022-06-07T03:47:20.162039Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def label_final (row):\n   if row['Predicted_NER_Tags'] == 'O' :\n      return 'O'\n   if row['Predicted_NER_Tags'] != 'O' and row['Predicted_Negation'] == True :\n      return \"I-N-\"+row['Predicted_NER_Tags']\n   if row['Predicted_NER_Tags'] != 'O' and row['Predicted_Negation'] == False :\n      return \"I-\"+row['Predicted_NER_Tags']","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:48:12.486021Z","iopub.execute_input":"2022-06-07T03:48:12.486761Z","iopub.status.idle":"2022-06-07T03:48:12.491694Z","shell.execute_reply.started":"2022-06-07T03:48:12.486722Z","shell.execute_reply":"2022-06-07T03:48:12.490881Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"final_valset['final_label'] = final_valset.apply (lambda row: label_final(row), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:48:14.806619Z","iopub.execute_input":"2022-06-07T03:48:14.807423Z","iopub.status.idle":"2022-06-07T03:48:14.889280Z","shell.execute_reply.started":"2022-06-07T03:48:14.807384Z","shell.execute_reply":"2022-06-07T03:48:14.888235Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"final_valset","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:48:17.403008Z","iopub.execute_input":"2022-06-07T03:48:17.403613Z","iopub.status.idle":"2022-06-07T03:48:17.421807Z","shell.execute_reply.started":"2022-06-07T03:48:17.403574Z","shell.execute_reply":"2022-06-07T03:48:17.420939Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"      Sentence_Index   Tokens Predicted_NER_Tags  Predicted_Negation  \\\n0                  0        I                  O               False   \n1                  0       am                  O               False   \n2                  0  looking                  O               False   \n3                  0      for                  O               False   \n4                  0        a                  O               False   \n...              ...      ...                ...                 ...   \n2345             166       30               SIZE               False   \n2346             166     inch               SIZE               False   \n2347             166    white             COLOUR               False   \n2348             166     desk            PRODUCT               False   \n2349             166      set            PRODUCT               False   \n\n     final_label  \n0              O  \n1              O  \n2              O  \n3              O  \n4              O  \n...          ...  \n2345      I-SIZE  \n2346      I-SIZE  \n2347    I-COLOUR  \n2348   I-PRODUCT  \n2349   I-PRODUCT  \n\n[2350 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence_Index</th>\n      <th>Tokens</th>\n      <th>Predicted_NER_Tags</th>\n      <th>Predicted_Negation</th>\n      <th>final_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I</td>\n      <td>O</td>\n      <td>False</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>am</td>\n      <td>O</td>\n      <td>False</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>looking</td>\n      <td>O</td>\n      <td>False</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>for</td>\n      <td>O</td>\n      <td>False</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>a</td>\n      <td>O</td>\n      <td>False</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>166</td>\n      <td>30</td>\n      <td>SIZE</td>\n      <td>False</td>\n      <td>I-SIZE</td>\n    </tr>\n    <tr>\n      <th>2346</th>\n      <td>166</td>\n      <td>inch</td>\n      <td>SIZE</td>\n      <td>False</td>\n      <td>I-SIZE</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>166</td>\n      <td>white</td>\n      <td>COLOUR</td>\n      <td>False</td>\n      <td>I-COLOUR</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>166</td>\n      <td>desk</td>\n      <td>PRODUCT</td>\n      <td>False</td>\n      <td>I-PRODUCT</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>166</td>\n      <td>set</td>\n      <td>PRODUCT</td>\n      <td>False</td>\n      <td>I-PRODUCT</td>\n    </tr>\n  </tbody>\n</table>\n<p>2350 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Validating the final results","metadata":{}},{"cell_type":"code","source":"clean_data = pd.read_csv(\"../input/nertag/clean_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:48:58.156604Z","iopub.execute_input":"2022-06-07T03:48:58.156874Z","iopub.status.idle":"2022-06-07T03:48:58.170614Z","shell.execute_reply.started":"2022-06-07T03:48:58.156843Z","shell.execute_reply":"2022-06-07T03:48:58.169524Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"clean_data = clean_data.replace('B-','I-', regex=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:52:54.365723Z","iopub.execute_input":"2022-06-07T03:52:54.365979Z","iopub.status.idle":"2022-06-07T03:52:54.378414Z","shell.execute_reply.started":"2022-06-07T03:52:54.365950Z","shell.execute_reply":"2022-06-07T03:52:54.377507Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"clean_data","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:52:56.805392Z","iopub.execute_input":"2022-06-07T03:52:56.805987Z","iopub.status.idle":"2022-06-07T03:52:56.821904Z","shell.execute_reply.started":"2022-06-07T03:52:56.805942Z","shell.execute_reply":"2022-06-07T03:52:56.821145Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"       Tokens       Tags  sentence\n0           I          O         0\n1          am          O         0\n2     looking          O         0\n3         for          O         0\n4           a          O         0\n...       ...        ...       ...\n2345       30     I-SIZE       166\n2346     inch     I-SIZE       166\n2347    white   I-COLOUR       166\n2348     desk  I-PRODUCT       166\n2349      set  I-PRODUCT       166\n\n[2350 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Tags</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>am</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>looking</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>for</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>O</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>30</td>\n      <td>I-SIZE</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2346</th>\n      <td>inch</td>\n      <td>I-SIZE</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>white</td>\n      <td>I-COLOUR</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>desk</td>\n      <td>I-PRODUCT</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>set</td>\n      <td>I-PRODUCT</td>\n      <td>166</td>\n    </tr>\n  </tbody>\n</table>\n<p>2350 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"clean_list = clean_data.Tags.values.tolist()\npred_list = final_valset.final_label.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:53:02.789787Z","iopub.execute_input":"2022-06-07T03:53:02.790074Z","iopub.status.idle":"2022-06-07T03:53:02.795278Z","shell.execute_reply.started":"2022-06-07T03:53:02.790044Z","shell.execute_reply":"2022-06-07T03:53:02.794282Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"print(classification_report(clean_list, pred_list))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:53:05.097118Z","iopub.execute_input":"2022-06-07T03:53:05.097656Z","iopub.status.idle":"2022-06-07T03:53:05.154629Z","shell.execute_reply.started":"2022-06-07T03:53:05.097617Z","shell.execute_reply":"2022-06-07T03:53:05.153792Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"               precision    recall  f1-score   support\n\n  I-ATTRIBUTE       0.65      0.72      0.68       148\n     I-COLOUR       0.86      0.65      0.74        48\n   I-MATERIAL       1.00      0.58      0.73        33\nI-N-ATTRIBUTE       0.85      0.57      0.68        98\n   I-N-COLOUR       1.00      0.25      0.40         8\n I-N-MATERIAL       0.50      0.25      0.33         4\n    I-N-PRICE       0.00      0.00      0.00         3\n  I-N-PRODUCT       0.45      0.48      0.47        21\n    I-N-SHAPE       0.00      0.00      0.00         2\n     I-N-SIZE       0.34      0.56      0.43        18\n  I-N-TEXTURE       0.00      0.00      0.00         4\n      I-PRICE       1.00      0.25      0.40        12\n    I-PRODUCT       0.92      0.87      0.90       320\n      I-SHAPE       1.00      0.44      0.62         9\n       I-SIZE       0.90      0.85      0.88       166\n    I-TEXTURE       1.00      0.12      0.22         8\n            O       0.92      0.98      0.95      1448\n\n     accuracy                           0.89      2350\n    macro avg       0.67      0.45      0.50      2350\n weighted avg       0.89      0.89      0.88      2350\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in zip(tokens,predictions):\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:00:06.324786Z","iopub.execute_input":"2022-06-06T07:00:06.325190Z","iopub.status.idle":"2022-06-06T07:00:06.633913Z","shell.execute_reply.started":"2022-06-06T07:00:06.325149Z","shell.execute_reply":"2022-06-06T07:00:06.633088Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('black', 'False')\n('gloss', 'False')\n('33', 'False')\n('inch', 'False')\n('fireclay', 'False')\n('apron', 'False')\n('sink', 'False')\n('Looking', 'False')\n('for', 'False')\n('pre', 'False')\n('workout', 'False')\n('Pump', 'False')\n('addict', 'False')\n('instead', 'False')\n('of', 'False')\n('Karbolyn', 'False')\n('Hydrate', 'False')\n('Wich', 'False')\n('one', 'False')\n('is', 'False')\n('better?', 'False')\n('i', 'False')\n('need', 'False')\n('a', 'False')\n('48', 'False')\n('inch', 'False')\n('glass', 'False')\n('sliding', 'False')\n('goof', 'False')\n('and', 'False')\n('a', 'False')\n('shower', 'False')\n('pan', 'False')\n('system', 'False')\n('for', 'False')\n('500.99', 'False')\n('or', 'False')\n('less', 'False')\n('Hello,', 'False')\n('do', 'False')\n('any', 'False')\n('of', 'False')\n('your', 'False')\n('free', 'False')\n('standing', 'False')\n('tubs', 'False')\n('have', 'False')\n('a', 'False')\n('matte', 'False')\n('white', 'False')\n('finish', 'False')\n('instead', 'False')\n('of', 'False')\n('a', 'False')\n('glossy', 'False')\n('finish?', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('24', 'False')\n('inch', 'False')\n('white', 'False')\n('mirror', 'False')\n('that', 'False')\n('is', 'False')\n('freestanding', 'False')\n('and', 'False')\n('reasonably', 'False')\n('priced.', 'False')\n('Does', 'False')\n('the', 'False')\n('linen', 'False')\n('add', 'False')\n('cotton', 'False')\n('duvet', 'False')\n('cover', 'False')\n('come', 'False')\n('in', 'False')\n('King?', 'False')\n('I', 'False')\n('saw', 'False')\n('it', 'False')\n('in', 'False')\n('the', 'False')\n('store', 'False')\n('but', 'False')\n('I', 'False')\n(\"don't\", 'False')\n('see', 'True')\n('the', 'True')\n('king', 'True')\n('size', 'True')\n('online', 'True')\n('?', 'False')\n('Hi', 'False')\n('there.', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('to', 'False')\n('buy', 'False')\n('some', 'False')\n('throw', 'False')\n('blankets.', 'False')\n('Your', 'False')\n('website', 'False')\n('states', 'False')\n('that', 'False')\n('the', 'False')\n('underside', 'False')\n('of', 'False')\n('the', 'False')\n('Boucle', 'False')\n('wool-like', 'False')\n('throw', 'False')\n('is', 'False')\n('very', 'False')\n('soft.', 'False')\n('I', 'False')\n('would', 'False')\n('like', 'False')\n('to', 'False')\n('know', 'False')\n('if', 'False')\n('the', 'False')\n('wool-like', 'False')\n('top', 'False')\n('side', 'False')\n('is', 'False')\n('soft', 'False')\n('as', 'False')\n('well,', 'False')\n('or', 'False')\n('if', 'False')\n('it', 'False')\n('is', 'False')\n('scratchy.', 'False')\n('I', 'False')\n('have', 'False')\n('hypersensitive', 'False')\n('skin', 'False')\n('and', 'False')\n('need', 'False')\n('a', 'False')\n('blanket', 'False')\n('that', 'False')\n('will', 'False')\n('not', 'False')\n('irritate', 'True')\n('me.', 'True')\n('Thanks.', 'False')\n('Long', 'False')\n('Sherpa', 'False')\n('jacket', 'False')\n('dark', 'False')\n('brown', 'False')\n('why', 'False')\n(\"can't\", 'False')\n('I', 'True')\n('find', 'True')\n('it?', 'True')\n('Hello.', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('soft', 'False')\n('Open', 'False')\n('face', 'False')\n('cardigan', 'False')\n('w', 'False')\n('no', 'False')\n('buttons.', 'True')\n('hi', 'False')\n('do', 'False')\n('you', 'False')\n('carry', 'False')\n('any', 'False')\n('oval', 'False')\n('tubs', 'False')\n('in', 'False')\n('a', 'False')\n('matte', 'False')\n('white', 'False')\n('finish', 'False')\n('or', 'False')\n('not', 'False')\n('too', 'True')\n('much', 'True')\n('of', 'True')\n('gloss?', 'True')\n('i', 'False')\n('know', 'False')\n('they', 'False')\n('make', 'False')\n('solid', 'False')\n('surface', 'False')\n('but', 'False')\n('higher', 'False')\n('prices.', 'False')\n('thank', 'False')\n('you', 'False')\n('Do', 'False')\n('you', 'False')\n('not', 'False')\n('have', 'True')\n('alcove', 'True')\n('shower', 'True')\n('walls?', 'True')\n('Hi', 'False')\n(\"I'm\", 'False')\n('considering', 'False')\n('a', 'False')\n('bathtub', 'False')\n('swivel', 'False')\n('panel.', 'False')\n('I', 'False')\n('assume', 'False')\n('it', 'False')\n(\"doesn't\", 'False')\n('matter', 'True')\n('which', 'True')\n('way', 'True')\n('the', 'True')\n('bathtub', 'True')\n('is', 'True')\n('facing?', 'True')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('alcove', 'False')\n('shower', 'False')\n('base', 'False')\n('.', 'False')\n('not', 'False')\n('so', 'True')\n('much', 'True')\n('$', 'True')\n('lol', 'True')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('a', 'False')\n('vanity', 'False')\n('tops', 'False')\n('or', 'False')\n('wall', 'False')\n('mount', 'False')\n('sink', 'False')\n('without', 'False')\n('faucet', 'True')\n('holes?', 'True')\n('Hi!!', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('60\"', 'False')\n('wide', 'False')\n('tub', 'False')\n('but', 'False')\n('a', 'False')\n('soaker', 'False')\n('type', 'False')\n('thats', 'False')\n('deeper', 'False')\n('than', 'False')\n('a', 'False')\n('regular', 'False')\n('tub', 'False')\n('do', 'False')\n('you', 'False')\n('have', 'False')\n('this', 'False')\n('type?', 'False')\n('I', 'False')\n(\"don't\", 'False')\n('see', 'True')\n('anything', 'True')\n('deep', 'True')\n('enough', 'True')\n('on', 'True')\n('your', 'True')\n('site', 'True')\n('Ok', 'False')\n('thanks.', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('pure', 'False')\n('white', 'False')\n('alcove', 'False')\n('bathtub,', 'False')\n('not', 'False')\n('American', 'True')\n('standard', 'True')\n('or', 'True')\n('any', 'True')\n('off', 'True')\n('whites.', 'True')\n('Is', 'False')\n('there', 'False')\n('one', 'False')\n('you', 'False')\n('could', 'False')\n('recommend', 'False')\n('Hi,', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('a', 'False')\n('navy', 'False')\n('mirror', 'False')\n('size', 'False')\n('60x31.', 'False')\n('You', 'False')\n('have', 'False')\n('it', 'False')\n('for', 'False')\n('the', 'False')\n('gold', 'False')\n('and', 'False')\n('black', 'False')\n('but', 'False')\n('i', 'False')\n(\"don't\", 'False')\n('want', 'True')\n('it', 'True')\n('for', 'True')\n('the', 'True')\n('navy.', 'True')\n('Hi,', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('floating', 'False')\n('vanity', 'False')\n('with', 'False')\n('integrated', 'False')\n('sink', 'False')\n('&', 'False')\n('counter', 'False')\n('top', 'False')\n('with', 'False')\n('no', 'False')\n('drill', 'True')\n('holes', 'True')\n('for', 'True')\n('the', 'True')\n('faucet', 'True')\n('as', 'True')\n('our', 'True')\n('faucets', 'True')\n('are', 'True')\n('wall', 'True')\n('mounted.', 'True')\n('Do', 'False')\n('you', 'False')\n('sell', 'False')\n('theM', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('soaker', 'False')\n('tub', 'False')\n('but', 'False')\n('do', 'False')\n('not', 'False')\n('want', 'True')\n('stand', 'True')\n('alone.', 'True')\n('I', 'False')\n('am', 'False')\n('replacing', 'False')\n('my', 'False')\n('oval', 'False')\n('jacuzzi', 'False')\n('Where', 'False')\n('is', 'False')\n('the', 'False')\n('mirror', 'False')\n('on', 'False')\n('here.', 'False')\n('I', 'False')\n(\"don't\", 'False')\n('want', 'True')\n('linen', 'True')\n('cabinet', 'True')\n('Hello,', 'False')\n('I', 'False')\n('need', 'False')\n('a', 'False')\n('standup', 'False')\n('shower', 'False')\n('door', 'False')\n('only,', 'False')\n('no', 'False')\n('side', 'True')\n('panels.', 'True')\n('41', 'False')\n('inches', 'False')\n('wide', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('double', 'False')\n('sink', 'False')\n('kitchen', 'False')\n('close', 'False')\n('to', 'False')\n('52', 'False')\n('inches', 'False')\n('no', 'False')\n('bigger', 'True')\n('with', 'True')\n('countertop', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('bypass', 'False')\n('shower', 'False')\n('doors', 'False')\n('in', 'False')\n('chrome', 'False')\n('with', 'False')\n('clear', 'False')\n('glass.', 'False')\n('The', 'False')\n('opening', 'False')\n('would', 'False')\n('be', 'False')\n('for', 'False')\n('a', 'False')\n('59\"', 'False')\n('width', 'False')\n('and', 'False')\n('the', 'False')\n('height', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('is', 'False')\n('76\".', 'False')\n('I', 'False')\n(\"don't\", 'False')\n('want', 'True')\n('any', 'True')\n('rubber', 'True')\n('seals', 'True')\n('around', 'True')\n('the', 'True')\n('glass', 'True')\n('doors.', 'True')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('at', 'False')\n('the', 'False')\n('34\"x34\"', 'False')\n('shower', 'False')\n('kit', 'False')\n('with', 'False')\n('door,', 'False')\n('walls,', 'False')\n('base', 'False')\n('and', 'False')\n('glass', 'False')\n('shelves.', 'False')\n('Can', 'False')\n('the', 'False')\n('2\"', 'False')\n('backsplash,', 'False')\n('on', 'False')\n('the', 'False')\n('60\"', 'False')\n('single', 'False')\n('sink', 'False')\n('countertop,', 'False')\n('be', 'False')\n('removed?', 'False')\n('Or', 'False')\n('is', 'False')\n('the', 'False')\n('counter', 'False')\n('and', 'False')\n('backsplash', 'False')\n('one', 'False')\n('piece?', 'False')\n('If', 'False')\n('so,', 'False')\n('do', 'False')\n('you', 'False')\n('have', 'False')\n('a', 'False')\n('single', 'False')\n('sink,', 'False')\n('white', 'False')\n('60\"', 'False')\n('vanity', 'False')\n('with', 'False')\n('no', 'False')\n('backsplash', 'True')\n('piece?', 'True')\n(\"We're\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('60', 'False')\n('inch', 'False')\n('left', 'False')\n('drain', 'False')\n('alcove', 'False')\n('tub', 'False')\n('which', 'False')\n('is', 'False')\n('a', 'False')\n('soaker', 'False')\n('tub', 'False')\n('with', 'False')\n('no', 'False')\n('more', 'True')\n('than', 'True')\n('15\"', 'True')\n('height.', 'True')\n('In', 'False')\n('white.', 'False')\n('just', 'False')\n('need', 'False')\n('glass', 'False')\n('doors', 'False')\n('no', 'False')\n('end', 'True')\n('glass', 'True')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('Chrome', 'False')\n('column-style', 'False')\n('showerhead', 'False')\n('with', 'False')\n('body', 'False')\n('jets', 'False')\n('and', 'False')\n('rain', 'False')\n('showerhead', 'False')\n('i', 'False')\n('live', 'False')\n('in', 'False')\n('__location__', 'False')\n('and', 'False')\n('i', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('an', 'False')\n('acrylic', 'False')\n('(', 'False')\n('preference', 'False')\n(')', 'False')\n('or', 'False')\n('fibreglass', 'False')\n('shower', 'False')\n('base.', 'False')\n('I', 'False')\n('need', 'False')\n('a', 'False')\n('50', 'False')\n('inch', 'False')\n('wide', 'False')\n('by', 'False')\n('34', 'False')\n('inch', 'False')\n('deep', 'False')\n('with', 'False')\n('a', 'False')\n('5', 'False')\n('inch', 'False')\n('single', 'False')\n('threshold', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('tub/shower', 'False')\n('righthand', 'False')\n('and', 'False')\n('one', 'False')\n('lefthand,', 'False')\n('what', 'False')\n('are', 'False')\n('my', 'False')\n('options?', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('freestanding', 'False')\n('tub', 'False')\n('with', 'False')\n('a', 'False')\n('bathing', 'False')\n('well', 'False')\n('bottom', 'False')\n('length', 'False')\n('of', 'False')\n('45', 'False')\n('inches', 'False')\n('minimum', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('round', 'False')\n('shower', 'False')\n('around', 'False')\n('73', 'False')\n('inches', 'False')\n('high', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('to', 'False')\n('buy', 'False')\n('a', 'False')\n('42\"', 'False')\n('gazebo', 'False')\n('with', 'False')\n('no', 'False')\n('more', 'True')\n('than', 'True')\n('21\"', 'True')\n('depth', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('shelves', 'False')\n('that', 'False')\n('mount', 'False')\n('to', 'False')\n('a', 'False')\n('wall,', 'False')\n('no', 'False')\n('screw', 'True')\n('30', 'True')\n('inches', 'True')\n('__brand__', 'False')\n('bathtub', 'False')\n('58', 'False')\n('in.', 'False')\n('x', 'False')\n('5', 'False')\n('in.', 'False')\n('4-Jet', 'False')\n('High', 'False')\n('Pressure', 'False')\n('Shower', 'False')\n('Panel', 'False')\n('System', 'False')\n('with', 'False')\n('Square', 'False')\n('Rain', 'False')\n('Shower', 'False')\n('Head', 'False')\n('and', 'False')\n('Tub', 'False')\n('Filler', 'False')\n('in', 'False')\n('Matte', 'False')\n('Black', 'False')\n('Model', 'False')\n('Yes,', 'False')\n('I', 'False')\n('bought', 'False')\n('a', 'False')\n('tub', 'False')\n('from', 'False')\n('you,', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('adjustable', 'False')\n('screw', 'False')\n('that', 'False')\n('attaches', 'False')\n('to', 'False')\n('the', 'False')\n('plug.', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('60', 'False')\n('inch', 'False')\n('white', 'False')\n('bathtub', 'False')\n('that', 'False')\n('is', 'False')\n('freestanding', 'False')\n('and', 'False')\n('reasonably', 'False')\n('priced', 'False')\n(\"i'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('unflavored', 'False')\n('mass', 'False')\n('gainer', 'False')\n('protein', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('a', 'False')\n('wall', 'False')\n('mount', 'False')\n('sink', 'False')\n('without', 'False')\n('a', 'True')\n('faucet', 'True')\n('hole', 'False')\n('Just', 'False')\n('shower', 'False')\n('base', 'False')\n('without', 'False')\n('walls', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('shower', 'False')\n('unit,', 'False')\n('60\"', 'False')\n('wide', 'False')\n('by', 'False')\n('42\"', 'False')\n('deep,', 'False')\n('with', 'False')\n('no', 'False')\n('seat', 'True')\n('and', 'True')\n('no', 'False')\n('top', 'True')\n('luggage', 'False')\n('without', 'False')\n('wheels', 'False')\n('I', 'False')\n('found', 'False')\n('a', 'False')\n('vanity', 'False')\n('top', 'False')\n('I', 'False')\n('love', 'False')\n('but', 'False')\n('it', 'False')\n('has', 'False')\n('only', 'False')\n('one', 'False')\n('hole', 'False')\n('for', 'False')\n('taps', 'False')\n('and', 'False')\n('that', 'False')\n(\"doesn't\", 'False')\n('suit', 'True')\n('our', 'True')\n('needs', 'True')\n('curbless', 'False')\n('shower', 'False')\n('Walk', 'False')\n('in', 'False')\n('tubs', 'False')\n('for', 'False')\n('seniors', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('shower', 'False')\n('door', 'False')\n('about', 'False')\n('30', 'False')\n('inches', 'False')\n('I', 'False')\n('need', 'False')\n('a', 'False')\n('toilet', 'False')\n('that', 'False')\n('is', 'False')\n('only', 'False')\n('28', 'False')\n('inches', 'False')\n('high', 'False')\n('Hi,', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('bathroom', 'False')\n('LED', 'False')\n('mirrors', 'False')\n('that', 'False')\n('are', 'False')\n('24', 'False')\n('inch.', 'False')\n('That', 'False')\n('are', 'False')\n('hard', 'False')\n('wired', 'False')\n('and', 'False')\n('can', 'False')\n('be', 'False')\n('turned', 'False')\n('on', 'False')\n('from', 'False')\n('the', 'False')\n('mirror', 'False')\n('Denim', 'False')\n('skirts?', 'False')\n('touch', 'False')\n('of', 'False')\n('linen', 'False')\n('buttoned', 'False')\n('skirt', 'False')\n('in', 'False')\n('patterned', 'False')\n('white', 'False')\n('colour', 'False')\n('Base', 'False')\n('cabinet', 'False')\n('42', 'False')\n('inches', 'False')\n('long', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('the', 'False')\n('sweater', 'False')\n('in', 'False')\n('medium?', 'False')\n('Hi.', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('sheer', 'False')\n('curtains', 'False')\n('which', 'False')\n('I', 'False')\n('can', 'False')\n('place', 'False')\n('on', 'False')\n('hooks', 'False')\n('instead', 'False')\n('of', 'False')\n('a', 'False')\n('rod', 'False')\n('Looking', 'False')\n('for', 'False')\n('Grey', 'False')\n('drapery', 'False')\n('panels', 'False')\n('10', 'False')\n('feet', 'False')\n('in', 'False')\n('length', 'False')\n('twik', 'False')\n('white', 'False')\n('sleeveless', 'False')\n('button', 'False')\n('closure', 'False')\n('top', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('waterproof', 'False')\n('queen', 'False')\n('size', 'False')\n('mattress', 'False')\n('protectors', 'False')\n('for', 'False')\n('over', 'False')\n('18', 'False')\n('inch', 'False')\n('mattresses?', 'False')\n('Looking', 'False')\n('for', 'False')\n('Mercerized', 'False')\n('cotton', 'False')\n('high-performance', 'False')\n('T-shirt', 'False')\n('in', 'False')\n('size', 'False')\n('medium', 'False')\n('Are', 'False')\n('the', 'False')\n('round', 'False')\n('velvet', 'False')\n('floor', 'False')\n('cushions', 'False')\n('available', 'False')\n('in', 'False')\n('store?', 'False')\n('I', 'False')\n('would', 'False')\n('like', 'False')\n('the', 'False')\n('sweater', 'False')\n('as', 'False')\n('well', 'False')\n('as', 'False')\n('the', 'False')\n('icons', 'False')\n('pencil', 'False')\n('skirt', 'False')\n('in', 'False')\n('green', 'False')\n('Sleeveless', 'False')\n('sweater', 'False')\n('Unflavored', 'False')\n('unsweetened', 'False')\n('do', 'False')\n('you', 'False')\n('have', 'False')\n('the', 'False')\n('stainless', 'False')\n('steel', 'False')\n('shaker', 'False')\n('cups', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('flavor', 'False')\n('powders', 'False')\n('which', 'False')\n('I', 'False')\n('can', 'False')\n('use', 'False')\n('in', 'False')\n('yoghurts', 'False')\n('for', 'False')\n('example', 'False')\n('Hi,', 'False')\n('do', 'False')\n('you', 'False')\n('sell', 'False')\n('fingerless', 'False')\n('gloves', 'False')\n('please', 'False')\n('Need', 'False')\n('an', 'False')\n('inexpensive', 'False')\n('thermostatic', 'False')\n('shower', 'False')\n('set', 'False')\n('with', 'False')\n('an', 'False')\n('extra', 'False')\n('outlet', 'False')\n('for', 'False')\n('bidet.', 'False')\n('Can', 'False')\n('you', 'False')\n('suggest', 'False')\n('a', 'False')\n('couple.', 'False')\n('hi,', 'False')\n('can', 'False')\n('I', 'False')\n('put', 'False')\n('a', 'False')\n('tankless', 'False')\n('toilet', 'False')\n('into', 'False')\n('a', 'False')\n('2\"x', 'False')\n('4\"', 'False')\n('wall?', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('left', 'False')\n('offset', 'False')\n('vanity', 'False')\n('set', 'False')\n('up', 'False')\n('to', 'False')\n('48', 'False')\n('inches', 'False')\n('wide,', 'False')\n('white,', 'False')\n('brushed', 'False')\n('nickel', 'False')\n('hardware', 'False')\n('seamless', 'False')\n('undermount', 'False')\n('Touchless', 'False')\n('kitchen', 'False')\n('faucet', 'False')\n('Looking', 'False')\n('for', 'False')\n('bowl', 'False')\n('with', 'False')\n('pattern', 'False')\n('LESS', 'False')\n('than', 'False')\n('5', 'False')\n('inches', 'False')\n('deep.', 'False')\n('Can', 'False')\n('be', 'False')\n('7', 'False')\n('inches', 'False')\n('wide', 'False')\n('Looking', 'False')\n('shower', 'False')\n('door', 'False')\n('frameless', 'False')\n('of', 'False')\n('60', 'False')\n('inch', 'False')\n('standing', 'False')\n('shower', 'False')\n('Do', 'False')\n('you', 'False')\n('sell', 'False')\n('beige', 'False')\n('toilets.', 'False')\n('Bone', 'False')\n('and', 'False')\n('buscuit', 'False')\n('are', 'False')\n('too', 'False')\n('light', 'False')\n('in', 'False')\n('colour', 'False')\n(\"We're\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('shower', 'False')\n('door', 'False')\n('set', 'False')\n('greater', 'False')\n('than', 'False')\n('70', 'False')\n('inches', 'False')\n('wide', 'False')\n('18', 'False')\n('inch', 'False')\n('deep', 'False')\n('by', 'False')\n('36', 'False')\n('inches', 'False')\n('long', 'False')\n('brown', 'False')\n('freestanding', 'False')\n('vanity', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('36', 'False')\n('inch', 'False')\n('vanity', 'False')\n('with', 'False')\n('granite', 'False')\n('top', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('bathroom', 'False')\n('mirrors', 'False')\n('black', 'False')\n('framed', 'False')\n('in', 'False')\n('rectangular', 'False')\n('and', 'False')\n('oval', 'False')\n('looking', 'False')\n('at', 'False')\n('36', 'False')\n('inch', 'False')\n('medicine', 'False')\n('cabinet', 'False')\n('for', 'False')\n('$399', 'False')\n('.', 'False')\n('is', 'False')\n('it', 'False')\n('recessed', 'False')\n('able', 'False')\n('to', 'False')\n('fit', 'False')\n('into', 'False')\n('wall', 'False')\n('as', 'False')\n('a', 'False')\n('flush', 'False')\n('mount?', 'False')\n('Vessel', 'False')\n('bag', 'False')\n('in', 'False')\n('goldish', 'False')\n('color', 'False')\n('Looking', 'False')\n('for', 'False')\n('protein', 'False')\n('powder', 'False')\n('no', 'False')\n('carb', 'True')\n('no', 'False')\n('suger', 'True')\n('I', 'False')\n('am', 'False')\n('brand', 'False')\n('new', 'False')\n('to', 'False')\n('fat', 'False')\n('burners,', 'False')\n('I', 'False')\n('want', 'False')\n('to', 'False')\n('know', 'False')\n('a', 'False')\n('good', 'False')\n('fat', 'False')\n('burner', 'False')\n('to', 'False')\n('see', 'False')\n('good', 'False')\n('results.', 'False')\n('i', 'False')\n('want', 'False')\n('protein', 'False')\n('without', 'False')\n('lactose', 'True')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('whey', 'False')\n('proteine', 'False')\n('with', 'False')\n('not', 'False')\n('soy', 'True')\n('in', 'True')\n('it!', 'True')\n('Hi,', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('whey', 'False')\n('protein', 'False')\n('with', 'False')\n('not', 'False')\n('soy,', 'True')\n('does', 'False')\n('it', 'False')\n('even', 'False')\n('exist?', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('supplement', 'False')\n('to', 'False')\n('help', 'False')\n('me', 'False')\n('get', 'False')\n('back', 'False')\n('into', 'False')\n('ketosis', 'False')\n(',', 'False')\n('but', 'False')\n('not', 'False')\n('a', 'True')\n('pill.', 'True')\n('Something', 'False')\n('I', 'False')\n('can', 'False')\n('mix', 'False')\n('in', 'False')\n('my', 'False')\n('water.', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('testosterone', 'False')\n('supplment', 'False')\n('and', 'False')\n('I', 'False')\n('have', 'False')\n('no', 'False')\n('idea', 'True')\n('which', 'True')\n('one', 'True')\n('to', 'True')\n('buy', 'True')\n('Hi,', 'False')\n('I', 'False')\n('would', 'False')\n('like', 'False')\n('to', 'False')\n('have', 'False')\n('a', 'False')\n('protein', 'False')\n('powder', 'False')\n('without', 'False')\n('caffeine', 'True')\n('i', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('very', 'False')\n('good', 'False')\n('fat', 'False')\n('burner', 'False')\n('thats', 'False')\n('not', 'False')\n('too', 'True')\n('heavy', 'True')\n('in', 'True')\n('stimulants.', 'True')\n('I', 'False')\n('saw', 'False')\n('a', 'False')\n('sweater', 'False')\n('block', 'False')\n('with', 'False')\n('multi', 'False')\n('colour', 'False')\n('yesterday.', 'False')\n('I', 'False')\n('wish', 'False')\n('to', 'False')\n('order', 'False')\n('and', 'False')\n('now', 'False')\n(\"can't\", 'False')\n('find..', 'True')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('polo', 'False')\n('shirt', 'False')\n('with', 'False')\n('no', 'False')\n('logo', 'True')\n('on', 'True')\n('it.', 'True')\n('Hi', 'False')\n('looking', 'False')\n('for', 'False')\n('the', 'False')\n('toque', 'False')\n('and', 'False')\n('neck', 'False')\n('tube', 'False')\n('displayed', 'False')\n('in', 'False')\n('the', 'False')\n(\"woman's\", 'False')\n('ski', 'False')\n('jacket', 'False')\n('the', 'False')\n('jacket', 'False')\n('color', 'False')\n('that', 'False')\n('is', 'False')\n('light', 'False')\n('blue', 'False')\n('and', 'False')\n('green', 'False')\n(',', 'False')\n('the', 'False')\n('tuque', 'False')\n('and', 'False')\n('neck', 'False')\n('tube', 'False')\n('is', 'False')\n('a', 'False')\n('dusty', 'False')\n('pink', 'False')\n('sort', 'False')\n('of', 'False')\n('color,', 'False')\n('I', 'False')\n(\"can't\", 'False')\n('find', 'True')\n('it', 'True')\n('anywhere', 'True')\n('on', 'True')\n('sight', 'True')\n('i', 'False')\n('see', 'True')\n('simiar', 'True')\n('items', 'True')\n('but', 'False')\n('want', 'False')\n('a', 'False')\n('matching', 'False')\n('set', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('bomber', 'False')\n('style', 'False')\n('jacket', 'False')\n('2', 'False')\n('XS', 'False')\n('no', 'False')\n('hoody', 'True')\n(',', 'False')\n('and', 'False')\n('black', 'False')\n(\"Women's\", 'False')\n('pyjama', 'False')\n('pants', 'False')\n('without', 'False')\n('waistband', 'False')\n('Good', 'False')\n('morning', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('puffer', 'False')\n('vest', 'False')\n('three-quarter', 'False')\n('length', 'False')\n(\"I've\", 'False')\n('seen', 'False')\n('them', 'False')\n('On', 'False')\n('the', 'False')\n('website', 'False')\n('but', 'False')\n('no', 'False')\n('longer', 'True')\n('there', 'True')\n('Good', 'False')\n('morning,', 'False')\n('I', 'False')\n('would', 'False')\n('like', 'False')\n('to', 'False')\n('buy', 'False')\n('the', 'False')\n('product', 'False')\n('below,', 'False')\n('size', 'False')\n('M:', 'False')\n('How', 'False')\n('can', 'False')\n('I', 'False')\n('proceed', 'False')\n('with', 'False')\n('this', 'False')\n('purchase?', 'False')\n('It', 'False')\n(\"doesn't\", 'False')\n('seem', 'True')\n('to', 'True')\n('be', 'True')\n('available', 'True')\n('at', 'True')\n('the', 'True')\n('moment', 'True')\n('Want', 'False')\n('to', 'False')\n('order', 'False')\n(\"Women's\", 'False')\n('warm', 'False')\n('waterproof', 'False')\n('snow', 'False')\n('boots', 'False')\n('in', 'False')\n('size', 'False')\n('8', 'False')\n('.', 'False')\n(\"Doesn't\", 'False')\n('say', 'True')\n('out', 'True')\n('of', 'True')\n('stock', 'True')\n('but', 'True')\n(\"can't\", 'False')\n('put', 'True')\n('order', 'True')\n('in.', 'True')\n('Please', 'False')\n('advise.', 'False')\n('Hi,', 'False')\n(\"I'm\", 'False')\n('interested', 'False')\n('in', 'False')\n('the', 'False')\n(\"women's\", 'False')\n('waterproof', 'False')\n('mountain', 'False')\n('walking', 'False')\n('jacket', 'False')\n('in', 'False')\n('green', 'False')\n('but', 'False')\n('the', 'False')\n('website', 'False')\n('only', 'False')\n('gives', 'False')\n('the', 'False')\n('option', 'False')\n('for', 'False')\n('size', 'False')\n('XS', 'False')\n('and', 'False')\n('does', 'False')\n('not', 'False')\n('allow', 'True')\n('to', 'True')\n('request', 'True')\n('a', 'True')\n('notification', 'True')\n('Hi', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('to', 'False')\n('buy', 'False')\n('this', 'False')\n('but', 'False')\n('I', 'False')\n('cannot', 'False')\n('find', 'True')\n('an', 'True')\n('22', 'True')\n('mm', 'True')\n('nozzle', 'True')\n('pump', 'True')\n('anywhere,', 'True')\n('do', 'False')\n('you', 'False')\n('have', 'False')\n('one', 'False')\n('on', 'False')\n('your', 'False')\n('site', 'False')\n('or', 'False')\n('know', 'False')\n('where', 'False')\n('I', 'False')\n('can', 'False')\n('get', 'False')\n('one?', 'False')\n('Thanks', 'False')\n('Pool', 'False')\n('cue', 'False')\n('cases', 'False')\n('not', 'False')\n('pool', 'True')\n('cues', 'False')\n('Looking', 'False')\n('to', 'False')\n('buy', 'False')\n('hiking', 'False')\n('trousers', 'False')\n('but', 'False')\n(\"don't\", 'False')\n('understand', 'True')\n('sizes.', 'True')\n('I', 'False')\n('am', 'False')\n('38in', 'False')\n('waist', 'False')\n('and', 'False')\n('29in', 'False')\n('leg.', 'False')\n('Looking', 'False')\n('to', 'False')\n('buy', 'False')\n('trekking', 'False')\n('trousers', 'False')\n('what', 'False')\n('size', 'False')\n('do', 'False')\n('I', 'False')\n('need', 'False')\n('is', 'False')\n('it', 'False')\n('possible', 'False')\n('to', 'False')\n('order', 'False')\n('a', 'False')\n('bathroom', 'False')\n('vanity', 'False')\n('without', 'False')\n('the', 'True')\n('countertop?', 'True')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('shower', 'False')\n('unit', 'False')\n('including', 'False')\n('walls', 'False')\n('bae', 'False')\n('and', 'False')\n('glass', 'False')\n('enclosure', 'False')\n('but', 'False')\n('the', 'False')\n('whole', 'False')\n('unit', 'False')\n('cannot', 'False')\n('be', 'True')\n('higher', 'True')\n('than', 'True')\n('75', 'True')\n('inches', 'True')\n('Hi', 'False')\n('i', 'False')\n('need', 'False')\n('a', 'False')\n('10\"', 'False')\n('rough', 'False')\n('in', 'False')\n('toilet', 'False')\n('asap,', 'False')\n(\"i'm\", 'False')\n('not', 'False')\n('sure', 'True')\n('if', 'True')\n('you', 'True')\n('have', 'True')\n('it', 'True')\n('in', 'True')\n('stock.', 'True')\n('I', 'False')\n(\"don't\", 'False')\n('want', 'True')\n('the', 'True')\n('screw', 'True')\n('less', 'True')\n('Can', 'False')\n('the', 'False')\n('shower', 'False')\n('be', 'False')\n('sold', 'False')\n('without', 'False')\n('the', 'True')\n('walls?', 'True')\n('You', 'False')\n(\"don't\", 'False')\n('sell', 'True')\n('any', 'True')\n('72\"', 'True')\n('wide', 'True')\n('vanities?', 'True')\n('looking', 'False')\n('for', 'False')\n('shower', 'False')\n('system', 'False')\n('with', 'False')\n('tub', 'False')\n('faucet', 'False')\n('altough', 'False')\n('no', 'False')\n('tub', 'True')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('bathroom', 'False')\n('vanity', 'False')\n('&', 'False')\n('sink', 'False')\n('no', 'False')\n('more', 'True')\n('than', 'True')\n('16\"', 'False')\n('deep', 'False')\n('looking', 'False')\n('for', 'False')\n('48', 'False')\n('inches', 'False')\n('floor', 'False')\n('cabinet', 'False')\n('single', 'False')\n('sink,', 'False')\n('but', 'False')\n('no', 'False')\n('couter', 'True')\n('top', 'False')\n('Can', 'False')\n('I', 'False')\n('purchase', 'False')\n('shower', 'False')\n('doors', 'False')\n('and', 'False')\n('not', 'False')\n('the', 'True')\n('entire', 'True')\n('shower', 'True')\n('kit?', 'True')\n('Medicine', 'False')\n('cabinets', 'False')\n('with', 'False')\n('no', 'False')\n('mirror', 'True')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('55', 'False')\n('inch', 'False')\n('vanity', 'False')\n('top,', 'False')\n('which', 'False')\n('is', 'False')\n('not', 'False')\n('a', 'True')\n('regular', 'True')\n('size.', 'True')\n('Is', 'False')\n('there', 'False')\n('a', 'False')\n('way', 'False')\n('to', 'False')\n('have', 'False')\n('this', 'False')\n('size?', 'False')\n('I', 'False')\n('need', 'False')\n('a', 'False')\n('42\"', 'False')\n('kitchen', 'False')\n('island.', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('any?', 'False')\n('I', 'False')\n(\"can't\", 'False')\n('find', 'True')\n('them', 'True')\n('on', 'True')\n('the', 'True')\n('website.', 'True')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('black', 'False')\n('glass', 'False')\n('shower', 'False')\n('tile.', 'False')\n('It', 'False')\n('does', 'False')\n('not', 'False')\n('need', 'True')\n('to', 'True')\n('be', 'True')\n('a', 'True')\n('solid', 'True')\n('black,', 'True')\n('can', 'False')\n('have', 'False')\n('accents', 'False')\n('of', 'False')\n('clear', 'False')\n('or', 'False')\n('silver', 'False')\n('glass', 'False')\n('tones', 'False')\n('Medicine', 'False')\n('cabinet', 'False')\n('with', 'False')\n('mirror', 'False')\n('and', 'False')\n('no', 'False')\n('lights', 'True')\n('We', 'False')\n('are', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('72', 'False')\n('inch', 'False')\n('soaker', 'False')\n('tub,', 'False')\n('no', 'False')\n('jets.', 'True')\n('can', 'False')\n('I', 'False')\n('order', 'False')\n('just', 'False')\n('the', 'False')\n('top', 'False')\n('and', 'False')\n('not', 'False')\n('the', 'True')\n('cabinet?', 'True')\n('barbeque', 'False')\n('without', 'False')\n('charcoal', 'False')\n('Hi', 'False')\n('there,', 'False')\n(\"I'm\", 'False')\n('interested', 'False')\n('in', 'False')\n('a', 'False')\n('vanity.', 'False')\n('Am', 'False')\n('I', 'False')\n('able', 'False')\n('to', 'False')\n('purchase', 'False')\n('without', 'False')\n('the', 'False')\n('sink', 'True')\n('top?', 'True')\n('Can', 'False')\n('I', 'False')\n('order', 'False')\n('this', 'False')\n('vessel', 'False')\n('sink', 'False')\n('without', 'False')\n('the', 'True')\n('attached', 'True')\n('faucet', 'True')\n('?', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('30\"', 'False')\n('cabinet', 'False')\n('with', 'False')\n('no', 'False')\n('top', 'True')\n('Looking', 'False')\n('for', 'False')\n('vanity', 'False')\n('but', 'False')\n('sink', 'False')\n('must', 'False')\n('have', 'False')\n('no', 'False')\n('holes.', 'True')\n('Faucet', 'False')\n('in', 'False')\n('on', 'False')\n('wall', 'False')\n('already', 'False')\n('Hi.', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('36\"', 'False')\n('vanity', 'False')\n('wall', 'False')\n('mounted', 'False')\n('black', 'False')\n('or', 'False')\n('white.', 'False')\n('With', 'False')\n('towel', 'False')\n('storage', 'False')\n('but', 'False')\n('no', 'False')\n('sinc', 'True')\n('I', 'False')\n('want', 'False')\n('just', 'False')\n('this', 'False')\n('cabinet', 'False')\n('without', 'False')\n('the', 'True')\n('top', 'True')\n('as', 'False')\n('i', 'False')\n('want', 'False')\n('top', 'False')\n('mounted', 'False')\n('basins', 'False')\n('on', 'False')\n('a', 'False')\n('surface', 'False')\n('matching', 'False')\n('my', 'False')\n('kitchen', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('pivot', 'False')\n('shower', 'False')\n('door', 'False')\n('in', 'False')\n('black', 'False')\n('to', 'False')\n('fit', 'False')\n('a', 'False')\n('33', 'False')\n('inch', 'False')\n('opening', 'False')\n('preferably', 'False')\n('but', 'False')\n('not', 'False')\n('necessarily.', 'True')\n('bathtub', 'False')\n('without', 'False')\n('faucet?', 'False')\n('Can', 'False')\n('you', 'False')\n('order', 'False')\n('the', 'False')\n('59\"', 'False')\n('bathrub', 'False')\n('without', 'False')\n('the', 'True')\n('faucet?', 'True')\n('I', 'False')\n('want', 'False')\n('Grey', 'False')\n('toilet', 'False')\n('not', 'False')\n('assessceries', 'True')\n('I', 'False')\n('cannot', 'False')\n('seem', 'True')\n('to', 'True')\n('sort', 'True')\n('by', 'True')\n('colour.', 'True')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('cream', 'False')\n('38\"', 'False')\n('corner', 'False')\n('shower', 'False')\n('stall', 'False')\n('do', 'False')\n('you', 'False')\n('have', 'False')\n('60', 'False')\n('Inch', 'False')\n('kitchen', 'False')\n('island', 'False')\n('with', 'False')\n('quartz', 'False')\n('top', 'False')\n('with', 'False')\n('no', 'False')\n('seatings', 'True')\n('Frameless', 'False')\n('beveled', 'False')\n('mirror', 'False')\n('no', 'False')\n('more', 'True')\n('than', 'True')\n('29\"x29\"', 'True')\n('Bath', 'False')\n('tub', 'False')\n('without', 'False')\n('texture', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('bathroom', 'False')\n('cabinet', 'False')\n('without', 'False')\n('top', 'True')\n('60', 'True')\n('inches', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('medicine', 'False')\n('cabinet', 'False')\n('with', 'False')\n('mirror', 'False')\n('front', 'False')\n('not', 'False')\n('Recessed', 'True')\n('that', 'True')\n('would', 'True')\n('fit', 'True')\n('50', 'True')\n('inch', 'True')\n('wide', 'True')\n('39', 'False')\n('inch', 'False')\n('tall', 'False')\n('the', 'False')\n('best', 'False')\n('I', 'False')\n('need', 'False')\n('a', 'False')\n('60\"', 'False')\n('vanity', 'False')\n('with', 'False')\n('a', 'False')\n('sink', 'False')\n(\"that's\", 'False')\n('offset', 'False')\n('to', 'False')\n('the', 'False')\n('right', 'False')\n('-', 'False')\n('not', 'False')\n('centred', 'True')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('37', 'False')\n('inch', 'False')\n('bathroom', 'False')\n('cabinet', 'False')\n('without', 'False')\n('a', 'True')\n('top', 'True')\n('or', 'True')\n('sink', 'True')\n('because', 'False')\n('I', 'False')\n('have', 'False')\n('one.', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('anything', 'False')\n('in', 'False')\n('stock?', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('mirror', 'False')\n('without', 'False')\n('a', 'True')\n('fram', 'True')\n('40', 'True')\n('inches', 'True')\n('x', 'True')\n('60', 'True')\n('inches', 'True')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('kitchen', 'False')\n('faucet', 'False')\n('in', 'False')\n('Stainless', 'False')\n('Steel.', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('in', 'False')\n('stock', 'False')\n('na', 'False')\n('did', 'False')\n('not', 'False')\n('what', 'True')\n('would', 'True')\n('be', 'True')\n('turn', 'True')\n('around', 'True')\n('time', 'True')\n('and', 'True')\n('pricing.', 'True')\n('Thanks', 'False')\n('Hi', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('24', 'False')\n('inch', 'False')\n('vanity', 'False')\n('top', 'False')\n('and', 'False')\n('sink,', 'False')\n('no', 'False')\n('base', 'True')\n('Bathroom', 'False')\n('vanities', 'False')\n('no', 'False')\n('countertop', 'True')\n('I', 'False')\n(\"don't\", 'False')\n('want', 'True')\n('double', 'True')\n('sink,', 'True')\n('I', 'False')\n('want', 'False')\n('a', 'False')\n('rectangle', 'False')\n('sink', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('30', 'False')\n('inch', 'False')\n('bathroom', 'False')\n('vanity', 'False')\n('not', 'False')\n('55\"', 'True')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('30', 'False')\n('inch', 'False')\n('bathroom', 'False')\n('shelf', 'False')\n('not', 'False')\n('wall', 'True')\n('mounted', 'True')\n('Wall', 'False')\n('mount', 'False')\n('rectangular', 'False')\n('sink', 'False')\n('without', 'False')\n('a', 'True')\n('faucet', 'True')\n('hole', 'False')\n('Grey', 'False')\n('toilet', 'False')\n('not', 'False')\n('toilet', 'True')\n('brush', 'False')\n('Good', 'False')\n('morning.', 'False')\n('We', 'False')\n('are', 'False')\n('looking', 'False')\n('for', 'False')\n('chrome', 'False')\n('shower', 'False')\n('head', 'False')\n('with', 'False')\n('high', 'False')\n('pressure', 'False')\n('-', 'False')\n('simple', 'False')\n('no', 'False')\n('bells', 'True')\n('and', 'True')\n('whistles', 'False')\n('just', 'False')\n('good', 'False')\n('pressure', 'False')\n('can', 'False')\n('you', 'False')\n('help', 'False')\n('me', 'False')\n('find', 'False')\n('a', 'False')\n('pharmacy', 'False')\n('without', 'False')\n('a', 'True')\n('mirror', 'True')\n('18-20\"', 'False')\n('wide?', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('double', 'False')\n('sink', 'False')\n('in', 'False')\n('black', 'False')\n('that', 'False')\n('is', 'False')\n('no', 'False')\n('more', 'True')\n('than', 'True')\n('23\"', 'True')\n('wide.', 'True')\n('Does', 'False')\n('this', 'False')\n('exist?', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('toilet', 'False')\n('seat', 'False')\n('without', 'False')\n('undermounted', 'True')\n('hinges', 'True')\n('for', 'True')\n('a', 'True')\n('one', 'True')\n('piece', 'True')\n('toilet', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('mirror', 'False')\n('medicine', 'False')\n('cabinet', 'False')\n('without', 'False')\n('led', 'False')\n('Do', 'False')\n('you', 'False')\n('have', 'False')\n('shower', 'False')\n('panels', 'False')\n('that', 'False')\n(\"aren't\", 'False')\n('see', 'True')\n('through?', 'True')\n('Like', 'False')\n('frosted', 'False')\n('glass', 'False')\n('I', 'False')\n(\"don't\", 'False')\n('need', 'True')\n('a', 'True')\n('vanity', 'True')\n('just', 'True')\n('the', 'True')\n('quartz', 'True')\n('top', 'True')\n('with', 'True')\n('built', 'True')\n('in', 'True')\n('sink', 'True')\n('Hi,', 'False')\n('I', 'False')\n('was', 'False')\n('looking', 'False')\n('a', 'False')\n('free', 'False')\n('standing', 'False')\n('tub', 'False')\n('yesterday', 'False')\n('for', 'False')\n('$650', 'False')\n(',', 'False')\n('it', 'False')\n('was', 'False')\n('on', 'False')\n('for', 'False')\n('50%', 'False')\n('off.', 'False')\n('I', 'False')\n(\"don't\", 'False')\n('see', 'True')\n('it', 'True')\n('now,', 'True')\n('is', 'False')\n('it', 'False')\n('still', 'False')\n('on', 'False')\n('sale', 'False')\n('?', 'False')\n('Looking', 'False')\n('for', 'False')\n('a', 'False')\n('large', 'False')\n('free', 'False')\n('standing', 'False')\n('tub...', 'False')\n('not', 'False')\n('acrylic', 'True')\n('or', 'True')\n('fiberglass', 'True')\n('i', 'False')\n(\"don't\", 'False')\n('want', 'True')\n('a', 'True')\n('complete', 'True')\n('vanity', 'True')\n('just', 'True')\n('the', 'True')\n('top', 'True')\n('Hi', 'False')\n('..', 'False')\n('never', 'False')\n('been', 'True')\n('to', 'True')\n('your', 'True')\n('store', 'True')\n(',looking', 'True')\n('for', 'True')\n('vinyl', 'True')\n('wardrobes', 'True')\n('do', 'False')\n('not', 'False')\n('know', 'True')\n('if', 'True')\n('you', 'True')\n('carry', 'True')\n('those', 'True')\n('..', 'False')\n('thanks', 'False')\n('Thank', 'False')\n('you', 'False')\n('very', 'False')\n('much', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('shower', 'False')\n('stall', 'False')\n('kit', 'False')\n('that', 'False')\n('is', 'False')\n('not', 'False')\n('priced', 'True')\n('to', 'True')\n('high', 'True')\n(\"it's\", 'False')\n('for', 'False')\n('a', 'False')\n('basement', 'False')\n('t-shirt', 'False')\n('without', 'False')\n('logo', 'False')\n('Shower', 'False')\n('door', 'False')\n('no', 'False')\n('side', 'True')\n('panels', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('heated', 'False')\n('air', 'False')\n('and', 'False')\n('water', 'False')\n('jetted', 'False')\n('tub.', 'False')\n('bathub', 'False')\n('with', 'False')\n('jets', 'False')\n('Hello,', 'False')\n('what', 'False')\n('is', 'False')\n('the', 'False')\n('biggest', 'False')\n('blade', 'False')\n('on', 'False')\n('a', 'False')\n('shower', 'False')\n('squeegee', 'False')\n('that', 'False')\n('you', 'False')\n('carry??', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('bathtub', 'False')\n('left', 'False')\n('hand', 'False')\n('drain', 'False')\n('but', 'False')\n('square', 'False')\n('not', 'False')\n('oval', 'True')\n('Do', 'False')\n('you', 'False')\n('sell', 'False')\n('a', 'False')\n('tub', 'False')\n('shower', 'False')\n('faucet', 'False')\n('set', 'False')\n('that', 'False')\n('does', 'False')\n('not', 'False')\n('have', 'True')\n('the', 'True')\n('anti', 'True')\n('scald', 'True')\n('feature?', 'True')\n('What', 'False')\n('rectangular', 'False')\n('shower', 'False')\n('units', 'False')\n('are', 'False')\n('available?', 'False')\n('I', 'False')\n('am', 'False')\n('looking', 'False')\n('for', 'False')\n('a', 'False')\n('35', 'False')\n('inch', 'False')\n('bathroom', 'False')\n('sink', 'False')\n('countertop', 'False')\n('with', 'False')\n('sink', 'False')\n(\"I'm\", 'False')\n('looking', 'False')\n('f', 'False')\n('or', 'False')\n('a', 'False')\n('Black', 'False')\n('Matte', 'False')\n('bathtub', 'False')\n('double', 'False')\n('sliding', 'False')\n('door', 'False')\n('Need', 'False')\n('a', 'False')\n('27', 'False')\n('inch', 'False')\n('frameless', 'False')\n('shower', 'False')\n('door', 'False')\n('Looking', 'False')\n('for', 'False')\n('30', 'False')\n('inch', 'False')\n('white', 'False')\n('desk', 'False')\n('set', 'False')\n","output_type":"stream"}]}]}